{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "07OhpBLDqTIf"
   },
   "source": [
    "# The Price Is Right: Predicting Prices with Product Images\n",
    "\n",
    "### Milestone Report\n",
    "\n",
    "------------\n",
    "\n",
    "**Steven Chen, Edward Chou, Richard Yang**\n",
    "\n",
    "(Edward Chou and Richard Yang are not part of 230, but are part of 229.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Packages\n",
    "\n",
    "---------------\n",
    "\n",
    "For this project, we choose to use Keras with a Tensorflow backend. Keras is well suited for building complex CNNs, and we have experience with both Tensorflow and Keras from the CS230 programming assignments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "m5WvbQev1OuM"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from keras import applications\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dropout, Flatten, Dense, Input\n",
    "from keras.initializers import glorot_uniform\n",
    "\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialize the VGG-16 network without the final (top) layer, using the learned ImageNet weights. VGG-16 is a very deep CNN trained for object recognition on the ImageNet challenge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "LXDZTeHL1RRc"
   },
   "outputs": [],
   "source": [
    "# build the VGG16 network\n",
    "input_tensor = Input(shape=(224,224,3))\n",
    "model = applications.VGG16(weights='imagenet', include_top=False, input_tensor = input_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build our own layer on top of VGG. In particular, we flatten the final feature mapping of VGG-16 (consisting of 512 7 by 7 filters) into a single dimension. We then add a fully connected layer of 256 hidden units with ReLU activations, and use uniform Xavier initialization.\n",
    "\n",
    "We finish our model with an output layer of a single linear activation neuron, which will output the predicted price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "sQa_EjD81S0_"
   },
   "outputs": [],
   "source": [
    "# build a classifier model to put on top of the convolutional model\n",
    "top_model = Sequential()\n",
    "print(model.output_shape[1:])\n",
    "top_model.add(Flatten(input_shape=(model.output_shape[1:])))\n",
    "\n",
    "\n",
    "# Output layer\n",
    "# We do random weight intialization\n",
    "# Maybe this is why our loss is so bad?\n",
    "top_model.add(Dense(256, activation='relu', kernel_initializer='glorot_uniform'))\n",
    "top_model.add(Dense(1, activation='linear', name='output', kernel_initializer='glorot_uniform'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the pretrained VGG layers to be non-trainable so that we do spend time learning them. Instead, our learning will focus on the new layers we have added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "PIa-BP_J1jrM"
   },
   "outputs": [],
   "source": [
    "# add the model on top of the convolutional base\n",
    "new_model = Model(inputs= model.input, outputs = top_model(model.output))\n",
    "\n",
    "# set the first 19 layers (up to the last conv block)\n",
    "# to non-trainable (weights will not be updated)\n",
    "for layer in new_model.layers[:19]:\n",
    "    layer.trainable = False\n",
    "\n",
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we can see our added layer as sequential_2. Only our new layer is trainable: the rest are not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compile the model using mean squared error as the loss (since we are performing regression), and use an RMSprop optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "YrdQv9kyEpH-"
   },
   "outputs": [],
   "source": [
    "\n",
    "# SGD\n",
    "#new_model.compile(loss='mean_squared_error',\n",
    "#              optimizer=optimizers.SGD(lr=1e-4, momentum=0.9),\n",
    "#              metrics=['accuracy'])\n",
    "\n",
    "# RMSprop\n",
    "new_model.compile(loss='mean_squared_error',\n",
    "                  optimizer=optimizers.RMSprop(lr=0.01, rho=0.9, epsilon=1e-07, decay=0.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the great Keras ImageDataGenerator to process our images. We rescale the image colors to be between 0 and 1, then perform mean subtraction on each image channel, in order to help our images be more standardized and similar to images the VGG network has seen before.\n",
    "\n",
    "Our dataset relies on the bike images and the price csv to be in the root directory, because that is where FloydHub puts them. As of now, we read in the images into a large numpy array, then feed this into the network. We hit memory issues when trying to load all 20000 plus images, so for now we load a smaller subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# read the CSV into memory\n",
    "prices = []\n",
    "image_paths = []\n",
    "\n",
    "data_path = \"../datasets/bikes_im/\"\n",
    "with open(\"../datasets/bikes_filtered.csv\") as file:\n",
    "    reader = csv.reader(file)\n",
    "    i = -1\n",
    "    for row in reader:\n",
    "        i += 1\n",
    "        index = row[0]\n",
    "        name = row[1]\n",
    "        msrp = row[2]\n",
    "        \n",
    "        image_path = data_path + index + '.jpg'\n",
    "        image_paths.append(image_path)\n",
    "        prices.append(int(msrp))\n",
    "\n",
    "        \n",
    "def image_generator(indices, batch_size):\n",
    "\n",
    "    num_batches = int(len(indices) / batch_size)\n",
    "    \n",
    "    while True:\n",
    "        for batch_i in range(num_batches):\n",
    "            if batch_i == num_batches - 1:\n",
    "                # special case: return as many as possible\n",
    "                start_i = batch_i * batch_size\n",
    "                batch_indices = indices[start_i:]\n",
    "                \n",
    "                X = np.zeros((len(batch_indices), 224, 224, 3))\n",
    "                Y = np.zeros((len(batch_indices), 1))\n",
    "            \n",
    "            else:\n",
    "                start_i = batch_i * batch_size\n",
    "                end_i = start_i + batch_size\n",
    "\n",
    "                batch_indices = indices[start_i:end_i]\n",
    "\n",
    "                X = np.zeros((batch_size, 224, 224, 3))\n",
    "                Y = np.zeros((batch_size, 1))\n",
    "            \n",
    "            for i, index in enumerate(batch_indices):\n",
    "                img = image.load_img(image_paths[index], target_size=(224, 224))\n",
    "                X[i, :, :, :] = image.img_to_array(img)                \n",
    "                Y[i] = prices[index]\n",
    "            \n",
    "            # use vgg16 preprocessing\n",
    "            X = preprocess_input(X)\n",
    "            \n",
    "            yield (X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create random permutation of number of data points, then cut\n",
    "# into train/test split\n",
    "\n",
    "# we have 21843 bike images total.\n",
    "dataset_indices = np.random.permutation(21843)\n",
    "\n",
    "# 90% train, 10% test\n",
    "cutoff = int(len(dataset_indices) * 0.9)\n",
    "train_indices = dataset_indices[:cutoff]\n",
    "test_indices = dataset_indices[cutoff:]\n",
    "\n",
    "print(len(train_indices))\n",
    "print(len(test_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "minibatch_size = 32\n",
    "\n",
    "train_steps = math.ceil(len(train_indices) / minibatch_size)\n",
    "test_steps = math.ceil(len(test_indices) / minibatch_size)\n",
    "\n",
    "# fine-tune the model\n",
    "new_model.fit_generator(\n",
    "    image_generator(train_indices, minibatch_size),\n",
    "    steps_per_epoch=train_steps,\n",
    "    epochs=epochs,\n",
    "    validation_data=image_generator(test_indices, minibatch_size),\n",
    "    nb_val_samples=test_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "CNN_transfer.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
