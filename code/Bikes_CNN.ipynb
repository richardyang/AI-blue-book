{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "07OhpBLDqTIf"
   },
   "source": [
    "# The Price Is Right: Predicting Prices with Product Images\n",
    "\n",
    "### Milestone Report\n",
    "\n",
    "------------\n",
    "\n",
    "**Steven Chen, Edward Chou, Richard Yang**\n",
    "\n",
    "(Edward Chou and Richard Yang are not part of 230, but are part of 229.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Packages\n",
    "\n",
    "---------------\n",
    "\n",
    "For this project, we choose to use Keras with a Tensorflow backend. Keras is well suited for building complex CNNs, and we have experience with both Tensorflow and Keras from the CS230 programming assignments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "m5WvbQev1OuM"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from keras import applications\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dropout, Flatten, Dense, Input\n",
    "from keras.initializers import glorot_uniform\n",
    "\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialize the VGG-16 network without the final (top) layer, using the learned ImageNet weights. VGG-16 is a very deep CNN trained for object recognition on the ImageNet challenge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "LXDZTeHL1RRc"
   },
   "outputs": [],
   "source": [
    "# build the VGG16 network\n",
    "input_tensor = Input(shape=(224,224,3))\n",
    "model = applications.VGG16(weights='imagenet', include_top=False, input_tensor = input_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build our own layer on top of VGG. In particular, we flatten the final feature mapping of VGG-16 (consisting of 512 7 by 7 filters) into a single dimension. We then add a fully connected layer of 256 hidden units with ReLU activations, and use uniform Xavier initialization.\n",
    "\n",
    "We finish our model with an output layer of a single linear activation neuron, which will output the predicted price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "sQa_EjD81S0_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, 7, 512)\n"
     ]
    }
   ],
   "source": [
    "# build a classifier model to put on top of the convolutional model\n",
    "top_model = Sequential()\n",
    "print(model.output_shape[1:])\n",
    "top_model.add(Flatten(input_shape=(model.output_shape[1:])))\n",
    "\n",
    "\n",
    "# Output layer\n",
    "# We do random weight intialization\n",
    "# Maybe this is why our loss is so bad?\n",
    "top_model.add(Dense(256, activation='relu', kernel_initializer='glorot_uniform'))\n",
    "top_model.add(Dense(1, activation='linear', name='output', kernel_initializer='glorot_uniform'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the pretrained VGG layers to be non-trainable so that we do spend time learning them. Instead, our learning will focus on the new layers we have added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "PIa-BP_J1jrM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "sequential_1 (Sequential)    (None, 1)                 6423041   \n",
      "=================================================================\n",
      "Total params: 21,137,729\n",
      "Trainable params: 6,423,041\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# add the model on top of the convolutional base\n",
    "new_model = Model(inputs= model.input, outputs = top_model(model.output))\n",
    "\n",
    "# set the first 19 layers (up to the last conv block)\n",
    "# to non-trainable (weights will not be updated)\n",
    "for layer in new_model.layers[:19]:\n",
    "    layer.trainable = False\n",
    "\n",
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we can see our added layer as sequential_2. Only our new layer is trainable: the rest are not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compile the model using mean squared error as the loss (since we are performing regression), and use an RMSprop optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "YrdQv9kyEpH-"
   },
   "outputs": [],
   "source": [
    "\n",
    "# SGD\n",
    "#new_model.compile(loss='mean_squared_error',\n",
    "#              optimizer=optimizers.SGD(lr=1e-4, momentum=0.9),\n",
    "#              metrics=['accuracy'])\n",
    "\n",
    "# RMSprop\n",
    "new_model.compile(loss='mean_squared_error',\n",
    "                  optimizer=optimizers.RMSprop(lr=0.01, rho=0.9, epsilon=1e-07, decay=0.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the great Keras ImageDataGenerator to process our images. We rescale the image colors to be between 0 and 1, then perform mean subtraction on each image channel, in order to help our images be more standardized and similar to images the VGG network has seen before.\n",
    "\n",
    "Our dataset relies on the bike images and the price csv to be in the root directory, because that is where FloydHub puts them. As of now, we read in the images into a large numpy array, then feed this into the network. We hit memory issues when trying to load all 20000 plus images, so for now we load a smaller subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the CSV into memory\n",
    "prices = []\n",
    "image_paths = []\n",
    "\n",
    "data_path = \"../datasets/bikes_im/\"\n",
    "with open(\"../datasets/bikes_filtered.csv\") as file:\n",
    "    reader = csv.reader(file)\n",
    "    i = -1\n",
    "    for row in reader:\n",
    "        i += 1\n",
    "        index = row[0]\n",
    "        name = row[1]\n",
    "        msrp = row[2]\n",
    "        \n",
    "        image_path = data_path + index + '.jpg'\n",
    "        image_paths.append(image_path)\n",
    "        prices.append(int(msrp))\n",
    "\n",
    "        \n",
    "def image_generator(indices, batch_size):\n",
    "\n",
    "    num_batches = int(len(indices) / batch_size)\n",
    "    \n",
    "    while True:\n",
    "        for batch_i in range(num_batches):\n",
    "            if batch_i == num_batches - 1:\n",
    "                # special case: return as many as possible\n",
    "                start_i = batch_i * batch_size\n",
    "                batch_indices = indices[start_i:]\n",
    "                \n",
    "                X = np.zeros((len(batch_indices), 224, 224, 3))\n",
    "                Y = np.zeros((len(batch_indices), 1))\n",
    "            \n",
    "            else:\n",
    "                start_i = batch_i * batch_size\n",
    "                end_i = start_i + batch_size\n",
    "\n",
    "                batch_indices = indices[start_i:end_i]\n",
    "\n",
    "                X = np.zeros((batch_size, 224, 224, 3))\n",
    "                Y = np.zeros((batch_size, 1))\n",
    "            \n",
    "            for i, index in enumerate(batch_indices):\n",
    "                img = image.load_img(image_paths[index], target_size=(224, 224))\n",
    "                X[i, :, :, :] = image.img_to_array(img)                \n",
    "                Y[i] = prices[index]\n",
    "            \n",
    "            # use vgg16 preprocessing\n",
    "            X = preprocess_input(X)\n",
    "            \n",
    "            yield (X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19658\n",
      "2185\n"
     ]
    }
   ],
   "source": [
    "# create random permutation of number of data points, then cut\n",
    "# into train/test split\n",
    "\n",
    "# we have 21843 bike images total.\n",
    "dataset_indices = np.random.permutation(21843)\n",
    "\n",
    "# 90% train, 10% test\n",
    "cutoff = int(len(dataset_indices) * 0.9)\n",
    "train_indices = dataset_indices[:cutoff]\n",
    "test_indices = dataset_indices[cutoff:]\n",
    "\n",
    "print(len(train_indices))\n",
    "print(len(test_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Richard\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:13: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "  del sys.path[0]\n",
      "C:\\Users\\Richard\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:13: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<generator..., validation_data=<generator..., steps_per_epoch=308, validation_steps=35, epochs=30)`\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "308/308 [==============================] - 117s 381ms/step - loss: 540842.4561 - val_loss: 651981.6422\n",
      "Epoch 2/30\n",
      "308/308 [==============================] - 113s 367ms/step - loss: 474738.5926 - val_loss: 760778.0059\n",
      "Epoch 3/30\n",
      "308/308 [==============================] - 114s 369ms/step - loss: 429311.7272 - val_loss: 1042465.3137\n",
      "Epoch 4/30\n",
      "308/308 [==============================] - 114s 369ms/step - loss: 396911.2101 - val_loss: 859611.1897\n",
      "Epoch 5/30\n",
      "308/308 [==============================] - 114s 370ms/step - loss: 367633.9550 - val_loss: 796590.8828\n",
      "Epoch 6/30\n",
      "308/308 [==============================] - 114s 369ms/step - loss: 347921.5304 - val_loss: 1077588.6092\n",
      "Epoch 7/30\n",
      "308/308 [==============================] - 114s 369ms/step - loss: 335931.2361 - val_loss: 1154946.0012\n",
      "Epoch 8/30\n",
      "308/308 [==============================] - 114s 369ms/step - loss: 317071.2103 - val_loss: 733699.2476\n",
      "Epoch 9/30\n",
      "308/308 [==============================] - 114s 369ms/step - loss: 296944.1740 - val_loss: 1197293.8670\n",
      "Epoch 10/30\n",
      "308/308 [==============================] - 114s 369ms/step - loss: 294776.4802 - val_loss: 688101.3231\n",
      "Epoch 11/30\n",
      "308/308 [==============================] - 114s 369ms/step - loss: 275225.6304 - val_loss: 695377.9545\n",
      "Epoch 12/30\n",
      "308/308 [==============================] - 114s 369ms/step - loss: 269150.0003 - val_loss: 601447.9142\n",
      "Epoch 13/30\n",
      "308/308 [==============================] - 114s 369ms/step - loss: 262451.8078 - val_loss: 664503.6362\n",
      "Epoch 14/30\n",
      "308/308 [==============================] - 114s 369ms/step - loss: 248342.1752 - val_loss: 685870.0174\n",
      "Epoch 15/30\n",
      "308/308 [==============================] - 114s 369ms/step - loss: 246756.3432 - val_loss: 649562.4116\n",
      "Epoch 16/30\n",
      "308/308 [==============================] - 114s 369ms/step - loss: 237435.1256 - val_loss: 648324.7194\n",
      "Epoch 17/30\n",
      "308/308 [==============================] - 114s 369ms/step - loss: 223072.2265 - val_loss: 616464.6747\n",
      "Epoch 18/30\n",
      "308/308 [==============================] - 114s 369ms/step - loss: 231880.5940 - val_loss: 655173.2319\n",
      "Epoch 19/30\n",
      "308/308 [==============================] - 114s 369ms/step - loss: 218534.7930 - val_loss: 914586.3282\n",
      "Epoch 20/30\n",
      "308/308 [==============================] - 114s 369ms/step - loss: 214131.6124 - val_loss: 782458.5263\n",
      "Epoch 21/30\n",
      "308/308 [==============================] - 114s 369ms/step - loss: 217211.5276 - val_loss: 676878.1439\n",
      "Epoch 22/30\n",
      "308/308 [==============================] - 114s 369ms/step - loss: 207511.8002 - val_loss: 617682.9590\n",
      "Epoch 23/30\n",
      "308/308 [==============================] - 114s 369ms/step - loss: 201565.3848 - val_loss: 763360.7070\n",
      "Epoch 24/30\n",
      "308/308 [==============================] - 114s 369ms/step - loss: 194002.9624 - val_loss: 604049.7064\n",
      "Epoch 25/30\n",
      "308/308 [==============================] - 114s 369ms/step - loss: 195329.0057 - val_loss: 636432.6449\n",
      "Epoch 26/30\n",
      "308/308 [==============================] - 114s 369ms/step - loss: 185772.2949 - val_loss: 591223.4917\n",
      "Epoch 27/30\n",
      "308/308 [==============================] - 113s 368ms/step - loss: 183359.8210 - val_loss: 604828.5969\n",
      "Epoch 28/30\n",
      "308/308 [==============================] - 114s 369ms/step - loss: 188940.5153 - val_loss: 626307.0363\n",
      "Epoch 29/30\n",
      "308/308 [==============================] - 114s 369ms/step - loss: 180713.0416 - val_loss: 588206.8536\n",
      "Epoch 30/30\n",
      "308/308 [==============================] - 114s 369ms/step - loss: 177476.0922 - val_loss: 626836.1169\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x28e7c63a358>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 30\n",
    "minibatch_size = 64\n",
    "\n",
    "train_steps = math.ceil(len(train_indices) / minibatch_size)\n",
    "test_steps = math.ceil(len(test_indices) / minibatch_size)\n",
    "\n",
    "# fine-tune the model\n",
    "new_model.fit_generator(\n",
    "    image_generator(train_indices, minibatch_size),\n",
    "    steps_per_epoch=train_steps,\n",
    "    epochs=epochs,\n",
    "    validation_data=image_generator(test_indices, minibatch_size),\n",
    "    nb_val_samples=test_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a data point and run it through the neural network\n",
    "# Return the predicted value and calculate the MSE\n",
    "def evaluate(index):\n",
    "    msrp = prices[index]\n",
    "    path = image_paths[index]\n",
    "    img = image.load_img(path, target_size=(224, 224))\n",
    "    data = np.expand_dims(image.img_to_array(img), axis=0)\n",
    "    prediction = new_model.predict(data, msrp)\n",
    "    #prediction = new_model.predict(data, np.expand_dims(np.array(msrp), axis=0))\n",
    "    print(\"Bike index: \" + str(index))\n",
    "    print(\"Actual price: \" + str(msrp))\n",
    "    print(\"Predicted price: \" + str(prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bike index: 10153\n",
      "Actual price: 850\n",
      "Predicted price: [[ 1149.99157715]]\n",
      "[21278 12151 16623 ..., 14244  9020 10153]\n"
     ]
    }
   ],
   "source": [
    "evaluate(10153)\n",
    "print(test_indices)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "CNN_transfer.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
