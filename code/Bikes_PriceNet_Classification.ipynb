{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19658,)\n",
      "(2185,)\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras import applications\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Dropout, Flatten, Dense, Input\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "import keras.backend as K\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Flatten, Dropout, Concatenate, Activation\n",
    "from keras.layers import Convolution2D, MaxPooling2D, AveragePooling2D\n",
    "from keras.layers import GlobalMaxPooling2D, GlobalAveragePooling2D\n",
    "\n",
    "from keras.applications.imagenet_utils import decode_predictions\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "from keras_applications.imagenet_utils import _obtain_input_shape\n",
    "from keras.utils.data_utils import get_file\n",
    "\n",
    "WEIGHTS_PATH = 'https://github.com/wohlert/keras-squeezenet/releases/download/v0.1/squeezenet_weights.h5'\n",
    "\n",
    "def _fire(x, filters, name=\"fire\"):\n",
    "    sq_filters, ex1_filters, ex2_filters = filters\n",
    "    squeeze = Convolution2D(sq_filters, (1, 1), activation='relu', padding='same', name=name + \"squeeze1x1\")(x)\n",
    "    expand1 = Convolution2D(ex1_filters, (1, 1), activation='relu', padding='same', name=name + \"expand1x1\")(squeeze)\n",
    "    expand2 = Convolution2D(ex2_filters, (3, 3), activation='relu', padding='same', name=name + \"expand3x3\")(squeeze)\n",
    "    x = Concatenate(axis=-1, name=name)([expand1, expand2])\n",
    "    return x\n",
    "\n",
    "def SqueezeNet(include_top=True, weights=None, input_tensor=None, input_shape=None, pooling=None, classes=1000):\n",
    "\n",
    "    if weights not in {'imagenet', None}:\n",
    "        raise ValueError('The `weights` argument should be either '\n",
    "                         '`None` (random initialization) or `imagenet` '\n",
    "                         '(pre-training on ImageNet).')\n",
    "\n",
    "    if weights == 'imagenet' and include_top and classes != 1000:\n",
    "        raise ValueError('If using `weights` as imagenet with `include_top`'\n",
    "                         ' as true, `classes` should be 1000')\n",
    "    # Determine proper input shape\n",
    "    input_shape = _obtain_input_shape(input_shape,\n",
    "                                      default_size=224,\n",
    "                                      min_size=48,\n",
    "                                      data_format=K.image_data_format(),\n",
    "                                      require_flatten=include_top)\n",
    "\n",
    "    if input_tensor is None:\n",
    "        img_input = Input(shape=input_shape)\n",
    "    else:\n",
    "        if not K.is_keras_tensor(input_tensor):\n",
    "            img_input = Input(tensor=input_tensor, shape=input_shape)\n",
    "        else:\n",
    "            img_input = input_tensor\n",
    "\n",
    "    x = Convolution2D(64, kernel_size=(3, 3), strides=(2, 2), padding=\"same\", activation=\"relu\", name='conv1')(img_input)\n",
    "    x = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), name='maxpool1', padding=\"valid\")(x)\n",
    "\n",
    "    x = _fire(x, (16, 64, 64), name=\"fire2\")\n",
    "    y = _fire(x, (16, 64, 64), name=\"fire3\")\n",
    "    \n",
    "    x = keras.layers.add([x,y])\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    \n",
    "    x = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), name='maxpool3', padding=\"valid\")(x)\n",
    "\n",
    "    x = _fire(x, (32, 128, 128), name=\"fire4\")\n",
    "    y = _fire(x, (32, 128, 128), name=\"fire5\")\n",
    "    \n",
    "    x = keras.layers.add([x,y])\n",
    "    x = keras.layers.BatchNormalization()(x)   \n",
    "\n",
    "    x = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), name='maxpool5', padding=\"valid\")(x)\n",
    "\n",
    "    x = _fire(x, (48, 192, 192), name=\"fire6\")\n",
    "    y = _fire(x, (48, 192, 192), name=\"fire7\")\n",
    "    \n",
    "    x = keras.layers.add([x,y])\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "\n",
    "    x = _fire(x, (64, 256, 256), name=\"fire8\")\n",
    "    y = _fire(x, (64, 256, 256), name=\"fire9\")\n",
    "    \n",
    "    x = keras.layers.add([x,y])\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "\n",
    "    if include_top:\n",
    "        x = Dropout(0.5, name='dropout9')(x)\n",
    "\n",
    "        x = Convolution2D(classes, (1, 1), padding='valid', name='conv10')(x)\n",
    "        x = AveragePooling2D(pool_size=(13, 13), name='avgpool10')(x)\n",
    "        x = Flatten(name='flatten10')(x)\n",
    "        x = Activation(\"softmax\", name='softmax')(x)\n",
    "    else:\n",
    "        if pooling == \"avg\":\n",
    "            x = GlobalAveragePooling2D(name=\"avgpool10\")(x)\n",
    "        else:\n",
    "            x = GlobalMaxPooling2D(name=\"maxpool10\")(x)\n",
    "\n",
    "    model = Model(img_input, x, name=\"squeezenet\")\n",
    "\n",
    "    if weights == 'imagenet':\n",
    "        weights_path = get_file('squeezenet_weights.h5',\n",
    "                                WEIGHTS_PATH,\n",
    "                                cache_subdir='models')\n",
    "\n",
    "        model.load_weights(weights_path)\n",
    "\n",
    "    return model\n",
    "\n",
    "# read the CSV into memory\n",
    "prices = []\n",
    "image_paths = []\n",
    "\n",
    "data_path = \"../datasets/bikes_im/\"\n",
    "with open(\"../datasets/bikes_classified.csv\") as file:\n",
    "    reader = csv.reader(file)\n",
    "    i = -1\n",
    "    for row in reader:\n",
    "        i += 1\n",
    "        index = row[0]\n",
    "        name = row[1]\n",
    "        msrp = row[2]\n",
    "        label = row[3]\n",
    "        \n",
    "        image_path = data_path + index + '.jpg'\n",
    "        image_paths.append(image_path)\n",
    "        prices.append(str(label))\n",
    "        \n",
    "train_indices = np.load(\"bikes_train_indices.npy\")\n",
    "test_indices = np.load(\"bikes_test_indices.npy\")\n",
    "print(train_indices.shape)\n",
    "print(test_indices.shape)\n",
    "\n",
    "from imgaug import augmenters as iaa\n",
    "\n",
    "seq = iaa.Sequential([\n",
    "    iaa.Crop(px=(0, 16)), # crop images from each side by 0 to 16px (randomly chosen)\n",
    "    iaa.Fliplr(0.5), # horizontally flip 50% of the images\n",
    "    iaa.GaussianBlur(sigma=(0, 3.0)) # blur images with a sigma of 0 to 3.0\n",
    "])\n",
    "\n",
    "def image_generator(indices, batch_size):\n",
    "\n",
    "    num_batches = int(len(indices) / batch_size)\n",
    "    \n",
    "    while True:\n",
    "        for batch_i in range(num_batches):\n",
    "            if batch_i == num_batches - 1:\n",
    "                # special case: return as many as possible\n",
    "                start_i = batch_i * batch_size\n",
    "                batch_indices = indices[start_i:]\n",
    "                \n",
    "                X = np.zeros((len(batch_indices), 224, 224, 3))\n",
    "                Y = np.zeros((len(batch_indices), 4)) # Change to one-hot\n",
    "            \n",
    "            else:\n",
    "                start_i = batch_i * batch_size\n",
    "                end_i = start_i + batch_size\n",
    "\n",
    "                batch_indices = indices[start_i:end_i]\n",
    "\n",
    "                X = np.zeros((batch_size, 224, 224, 3))\n",
    "                Y = np.zeros((batch_size, 4)) # Change to one-hot\n",
    "            \n",
    "            for i, index in enumerate(batch_indices):\n",
    "                img = image.load_img(image_paths[index], target_size=(224, 224))\n",
    "                X[i, :, :, :] = image.img_to_array(img)\n",
    "                # Convert to 1 hot vector\n",
    "                p = prices[index]\n",
    "                if p == \"25\":\n",
    "                    Y[i,:] = np.array([1,0,0,0])\n",
    "                if p == \"50\":\n",
    "                    Y[i,:] = np.array([0,1,0,0])\n",
    "                if p == \"75\":\n",
    "                    Y[i,:] = np.array([0,0,1,0])\n",
    "                if p == \"100\":\n",
    "                    Y[i,:] = np.array([0,0,0,1])\n",
    "            \n",
    "            # use vgg16 preprocessing\n",
    "            X = preprocess_input(X)\n",
    "#             X = seq.augment_images(X)\n",
    "            \n",
    "            yield (X, Y)\n",
    "\n",
    "# Hyperparameters\n",
    "\n",
    "num_settings = 1\n",
    "\n",
    "hp_dropout = [0.5] * num_settings\n",
    "\n",
    "#RMSprop\n",
    "hp_lr = [0.005] * num_settings\n",
    "hp_rho = [0.9] * num_settings\n",
    "hp_epsilon = [1e-07] * num_settings\n",
    "hp_decay = [0.0] * num_settings\n",
    "\n",
    "# Number of hidden units\n",
    "hp_hidden = [256] * num_settings\n",
    "\n",
    "# Minibatch size\n",
    "hp_mbsize = [64] * num_settings\n",
    "\n",
    "num_epochs = 1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 224, 224, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                  (None, 112, 112, 64) 1792        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "maxpool1 (MaxPooling2D)         (None, 55, 55, 64)   0           conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "fire2squeeze1x1 (Conv2D)        (None, 55, 55, 16)   1040        maxpool1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "fire2expand1x1 (Conv2D)         (None, 55, 55, 64)   1088        fire2squeeze1x1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire2expand3x3 (Conv2D)         (None, 55, 55, 64)   9280        fire2squeeze1x1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire2 (Concatenate)             (None, 55, 55, 128)  0           fire2expand1x1[0][0]             \n",
      "                                                                 fire2expand3x3[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "fire3squeeze1x1 (Conv2D)        (None, 55, 55, 16)   2064        fire2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "fire3expand1x1 (Conv2D)         (None, 55, 55, 64)   1088        fire3squeeze1x1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire3expand3x3 (Conv2D)         (None, 55, 55, 64)   9280        fire3squeeze1x1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire3 (Concatenate)             (None, 55, 55, 128)  0           fire3expand1x1[0][0]             \n",
      "                                                                 fire3expand3x3[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 55, 55, 128)  0           fire2[0][0]                      \n",
      "                                                                 fire3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 55, 55, 128)  512         add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "maxpool3 (MaxPooling2D)         (None, 27, 27, 128)  0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire4squeeze1x1 (Conv2D)        (None, 27, 27, 32)   4128        maxpool3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "fire4expand1x1 (Conv2D)         (None, 27, 27, 128)  4224        fire4squeeze1x1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire4expand3x3 (Conv2D)         (None, 27, 27, 128)  36992       fire4squeeze1x1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire4 (Concatenate)             (None, 27, 27, 256)  0           fire4expand1x1[0][0]             \n",
      "                                                                 fire4expand3x3[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "fire5squeeze1x1 (Conv2D)        (None, 27, 27, 32)   8224        fire4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "fire5expand1x1 (Conv2D)         (None, 27, 27, 128)  4224        fire5squeeze1x1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire5expand3x3 (Conv2D)         (None, 27, 27, 128)  36992       fire5squeeze1x1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire5 (Concatenate)             (None, 27, 27, 256)  0           fire5expand1x1[0][0]             \n",
      "                                                                 fire5expand3x3[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 27, 27, 256)  0           fire4[0][0]                      \n",
      "                                                                 fire5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 27, 27, 256)  1024        add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "maxpool5 (MaxPooling2D)         (None, 13, 13, 256)  0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire6squeeze1x1 (Conv2D)        (None, 13, 13, 48)   12336       maxpool5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "fire6expand1x1 (Conv2D)         (None, 13, 13, 192)  9408        fire6squeeze1x1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire6expand3x3 (Conv2D)         (None, 13, 13, 192)  83136       fire6squeeze1x1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire6 (Concatenate)             (None, 13, 13, 384)  0           fire6expand1x1[0][0]             \n",
      "                                                                 fire6expand3x3[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "fire7squeeze1x1 (Conv2D)        (None, 13, 13, 48)   18480       fire6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "fire7expand1x1 (Conv2D)         (None, 13, 13, 192)  9408        fire7squeeze1x1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire7expand3x3 (Conv2D)         (None, 13, 13, 192)  83136       fire7squeeze1x1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire7 (Concatenate)             (None, 13, 13, 384)  0           fire7expand1x1[0][0]             \n",
      "                                                                 fire7expand3x3[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 13, 13, 384)  0           fire6[0][0]                      \n",
      "                                                                 fire7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 13, 13, 384)  1536        add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "fire8squeeze1x1 (Conv2D)        (None, 13, 13, 64)   24640       batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire8expand1x1 (Conv2D)         (None, 13, 13, 256)  16640       fire8squeeze1x1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire8expand3x3 (Conv2D)         (None, 13, 13, 256)  147712      fire8squeeze1x1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire8 (Concatenate)             (None, 13, 13, 512)  0           fire8expand1x1[0][0]             \n",
      "                                                                 fire8expand3x3[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "fire9squeeze1x1 (Conv2D)        (None, 13, 13, 64)   32832       fire8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "fire9expand1x1 (Conv2D)         (None, 13, 13, 256)  16640       fire9squeeze1x1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire9expand3x3 (Conv2D)         (None, 13, 13, 256)  147712      fire9squeeze1x1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire9 (Concatenate)             (None, 13, 13, 512)  0           fire9expand1x1[0][0]             \n",
      "                                                                 fire9expand3x3[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 13, 13, 512)  0           fire8[0][0]                      \n",
      "                                                                 fire9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 13, 13, 512)  2048        add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "sequential_1 (Sequential)       (None, 4)            394756      batch_normalization_4[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 1,122,372\n",
      "Trainable params: 1,119,812\n",
      "Non-trainable params: 2,560\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Richard\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:56: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "C:\\Users\\Richard\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:56: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<generator..., validation_steps=35, epochs=300, validation_data=<generator..., callbacks=[<keras.ca..., steps_per_epoch=35)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "35/35 [==============================] - 15s 441ms/step - loss: 1.2915 - acc: 0.3967 - val_loss: 1.0984 - val_acc: 0.4940\n",
      "Epoch 2/300\n",
      "35/35 [==============================] - 13s 364ms/step - loss: 1.0916 - acc: 0.5037 - val_loss: 0.9733 - val_acc: 0.5602\n",
      "Epoch 3/300\n",
      "35/35 [==============================] - 12s 341ms/step - loss: 0.9699 - acc: 0.5607 - val_loss: 0.8867 - val_acc: 0.6052\n",
      "Epoch 4/300\n",
      "35/35 [==============================] - 12s 356ms/step - loss: 0.8782 - acc: 0.6195 - val_loss: 0.7575 - val_acc: 0.7043\n",
      "Epoch 5/300\n",
      "35/35 [==============================] - 12s 338ms/step - loss: 0.7778 - acc: 0.6712 - val_loss: 0.7280 - val_acc: 0.6945\n",
      "Epoch 6/300\n",
      "35/35 [==============================] - 12s 344ms/step - loss: 0.6754 - acc: 0.7343 - val_loss: 0.7481 - val_acc: 0.6741\n",
      "Epoch 7/300\n",
      "35/35 [==============================] - 12s 341ms/step - loss: 0.5916 - acc: 0.7809 - val_loss: 0.5765 - val_acc: 0.7470\n",
      "Epoch 8/300\n",
      "35/35 [==============================] - 12s 338ms/step - loss: 0.4968 - acc: 0.8175 - val_loss: 0.4389 - val_acc: 0.8488\n",
      "Epoch 9/300\n",
      "35/35 [==============================] - 12s 345ms/step - loss: 0.3953 - acc: 0.8681 - val_loss: 0.3496 - val_acc: 0.8839\n",
      "Epoch 10/300\n",
      "35/35 [==============================] - 12s 342ms/step - loss: 0.3102 - acc: 0.9085 - val_loss: 0.3376 - val_acc: 0.8799\n",
      "Epoch 11/300\n",
      "35/35 [==============================] - 11s 327ms/step - loss: 0.2219 - acc: 0.9385 - val_loss: 0.3179 - val_acc: 0.8791\n",
      "Epoch 12/300\n",
      "35/35 [==============================] - 12s 332ms/step - loss: 0.1759 - acc: 0.9520 - val_loss: 0.2389 - val_acc: 0.9031\n",
      "Epoch 13/300\n",
      "35/35 [==============================] - 12s 349ms/step - loss: 0.1204 - acc: 0.9724 - val_loss: 0.1489 - val_acc: 0.9493\n",
      "Epoch 14/300\n",
      "35/35 [==============================] - 12s 339ms/step - loss: 0.0923 - acc: 0.9790 - val_loss: 0.0728 - val_acc: 0.9853\n",
      "Epoch 15/300\n",
      "35/35 [==============================] - 12s 338ms/step - loss: 0.0786 - acc: 0.9826 - val_loss: 0.3568 - val_acc: 0.8577\n",
      "Epoch 16/300\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.0449 - acc: 0.9904- ETA: 3s - loss: 0.0"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-8a98bf1c495e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mimage_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_indices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mminibatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m         \u001b[0mnb_val_samples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m         callbacks=[checkpoint])\n\u001b[0m\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[0;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1413\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1414\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1415\u001b[1;33m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1416\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1417\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m    228\u001b[0m                             \u001b[0mval_enqueuer_gen\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    229\u001b[0m                             \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 230\u001b[1;33m                             workers=0)\n\u001b[0m\u001b[0;32m    231\u001b[0m                     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m                         \u001b[1;31m# No need for try/except because\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[0;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mevaluate_generator\u001b[1;34m(self, generator, steps, max_queue_size, workers, use_multiprocessing, verbose)\u001b[0m\n\u001b[0;32m   1467\u001b[0m             \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1468\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1469\u001b[1;33m             verbose=verbose)\n\u001b[0m\u001b[0;32m   1470\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1471\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mevaluate_generator\u001b[1;34m(model, generator, steps, max_queue_size, workers, use_multiprocessing, verbose)\u001b[0m\n\u001b[0;32m    341\u001b[0m                                  \u001b[1;34m'or (x, y). Found: '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    342\u001b[0m                                  str(generator_output))\n\u001b[1;32m--> 343\u001b[1;33m             \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    344\u001b[0m             \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    345\u001b[0m             \u001b[0mouts_per_batch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtest_on_batch\u001b[1;34m(self, x, y, sample_weight)\u001b[0m\n\u001b[0;32m   1252\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1253\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_test_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1254\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1255\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2670\u001b[0m                     \u001b[1;34m'In order to feed symbolic tensors to a Keras model '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2671\u001b[0m                     'in TensorFlow, you need tensorflow 1.8 or higher.')\n\u001b[1;32m-> 2672\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2673\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_legacy_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2652\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2653\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m-> 2654\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2655\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2656\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    903\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 905\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    906\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1107\u001b[0m             \u001b[0mfeed_handles\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msubfeed_val\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1108\u001b[0m           \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1109\u001b[1;33m             \u001b[0mnp_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubfeed_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msubfeed_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1110\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1111\u001b[0m           if (not is_tensor_handle_feed and\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\numpy\\core\\numeric.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m    490\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m     \"\"\"\n\u001b[1;32m--> 492\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    493\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    494\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# store the results of each setting\n",
    "train_losses = np.zeros(num_settings)\n",
    "dev_losses = np.zeros(num_settings)\n",
    "\n",
    "for setting in range(num_settings):\n",
    "    model = SqueezeNet(include_top=True)\n",
    "    \n",
    "    model.layers.pop()\n",
    "    model.layers.pop()\n",
    "    model.layers.pop()\n",
    "    model.layers.pop()\n",
    "    model.layers.pop()\n",
    "    \n",
    "    # build a classifier model to put on top of the convolutional model\n",
    "    top_model = Sequential()\n",
    "    top_model.add(Convolution2D(256, (1, 1), padding='valid', name='top_conv', input_shape=(model.layers[-1].output_shape[1:])))\n",
    "    top_model.add(AveragePooling2D(pool_size=(5, 5), name='top_avgpool'))\n",
    "    top_model.add(Flatten(input_shape=(model.layers[-1].output_shape[1:]),name='top_flatten'))\n",
    "    top_model.add(Dropout(hp_dropout[setting], name='top_dropout'))\n",
    "    top_model.add(Dense(hp_hidden[setting], activation='relu', kernel_initializer='glorot_uniform', name='top_dense'))\n",
    "    top_model.add(Dense(4, activation='softmax', name='output', kernel_initializer='glorot_uniform'))\n",
    "    \n",
    "    # add the model on top of the convolutional base\n",
    "    new_model = Model(inputs= model.input, outputs = top_model(model.layers[-1].output))\n",
    "#     new_model = load_model(\"E:/output/bikes-cnn-PriceNet-Class-Aug/00500.hdf5\")\n",
    "    new_model.summary()\n",
    "    \n",
    "    # RMSprop optimizer\n",
    "#     new_model.compile(loss='mean_squared_error',\n",
    "#                       optimizer=optimizers.RMSprop(\n",
    "#                               lr=hp_lr[setting], \n",
    "#                               rho=hp_rho[setting], \n",
    "#                               epsilon=hp_epsilon[setting], \n",
    "#                               decay=hp_decay[setting]))\n",
    "    new_model.compile(loss='categorical_crossentropy', optimizer=optimizers.RMSprop(lr=0.0001, rho=0.9, epsilon=1e-07, decay=0.0), metrics=['accuracy'])\n",
    "    \n",
    "    \n",
    "    checkpoint_path = 'E:/output/bikes-cnn-PriceNet-Class-Aug/{epoch:05d}.hdf5'\n",
    "    \n",
    "    # keep a checkpoint\n",
    "    checkpoint = ModelCheckpoint(checkpoint_path, period=5)\n",
    "    \n",
    "    \n",
    "    minibatch_size = hp_mbsize[setting]\n",
    "\n",
    "    train_steps = math.ceil(len(train_indices) / minibatch_size)\n",
    "    test_steps = math.ceil(len(test_indices) / minibatch_size)\n",
    "\n",
    "    # fine-tune the model\n",
    "    history = new_model.fit_generator(\n",
    "        image_generator(test_indices, minibatch_size),\n",
    "        steps_per_epoch=test_steps,\n",
    "        epochs=300,\n",
    "        validation_data=image_generator(test_indices, minibatch_size),\n",
    "        nb_val_samples=test_steps,\n",
    "        callbacks=[checkpoint])\n",
    "    \n",
    "   \n",
    "    print(\"==========\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('Model Acc')\n",
    "plt.ylabel('acc')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['train', 'test'], loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['train', 'test'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = load_model('E:/output/bikes-cnn-PriceNet-Class-Aug/00110.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "true_label = []\n",
    "predicted_label = []\n",
    "for index in test_indices:\n",
    "    msrp = prices[index]\n",
    "    true_label.append(str(msrp))\n",
    "    \n",
    "    path = image_paths[index]\n",
    "    img = image.load_img(path, target_size=(224, 224))\n",
    "    data = np.expand_dims(image.img_to_array(img), axis=0)\n",
    "    \n",
    "    # Prediction outputs softmax vector\n",
    "    prediction = new_model.predict(data)\n",
    "    \n",
    "    # Set most confident prediction as label, and convert it to our price scale\n",
    "    label = np.argmax(prediction) * 25 + 25\n",
    "    predicted_label.append(str(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "print(\"Classification report:\\n%s\\n\"\n",
    "      % (classification_report(true_label, predicted_label)))\n",
    "print(\"Confusion matrix:\\n%s\" % confusion_matrix(true_label, predicted_label))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "top_conv (Conv2D)            (None, 13, 13, 256)       131328    \n",
      "_________________________________________________________________\n",
      "top_avgpool (AveragePooling2 (None, 2, 2, 256)         0         \n",
      "_________________________________________________________________\n",
      "top_flatten (Flatten)        (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "top_dropout (Dropout)        (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "top_dense (Dense)            (None, 256)               262400    \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 4)                 1028      \n",
      "=================================================================\n",
      "Total params: 394,756\n",
      "Trainable params: 394,756\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "new_model.layers[-1].summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
