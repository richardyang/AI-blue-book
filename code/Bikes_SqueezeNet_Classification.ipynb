{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19658,)\n",
      "(2185,)\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras import applications\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Dropout, Flatten, Dense, Input\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "import keras.backend as K\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Flatten, Dropout, Concatenate, Activation\n",
    "from keras.layers import Convolution2D, MaxPooling2D, AveragePooling2D\n",
    "from keras.layers import GlobalMaxPooling2D, GlobalAveragePooling2D\n",
    "\n",
    "from keras.applications.imagenet_utils import decode_predictions\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "from keras_applications.imagenet_utils import _obtain_input_shape\n",
    "from keras.utils.data_utils import get_file\n",
    "\n",
    "WEIGHTS_PATH = 'https://github.com/wohlert/keras-squeezenet/releases/download/v0.1/squeezenet_weights.h5'\n",
    "\n",
    "def _fire(x, filters, name=\"fire\"):\n",
    "    sq_filters, ex1_filters, ex2_filters = filters\n",
    "    squeeze = Convolution2D(sq_filters, (1, 1), activation='relu', padding='same', name=name + \"squeeze1x1\")(x)\n",
    "    expand1 = Convolution2D(ex1_filters, (1, 1), activation='relu', padding='same', name=name + \"expand1x1\")(squeeze)\n",
    "    expand2 = Convolution2D(ex2_filters, (3, 3), activation='relu', padding='same', name=name + \"expand3x3\")(squeeze)\n",
    "    x = Concatenate(axis=-1, name=name)([expand1, expand2])\n",
    "    return x\n",
    "\n",
    "def SqueezeNet(include_top=True, weights=\"imagenet\", input_tensor=None, input_shape=None, pooling=None, classes=1000):\n",
    "\n",
    "    if weights not in {'imagenet', None}:\n",
    "        raise ValueError('The `weights` argument should be either '\n",
    "                         '`None` (random initialization) or `imagenet` '\n",
    "                         '(pre-training on ImageNet).')\n",
    "\n",
    "    if weights == 'imagenet' and include_top and classes != 1000:\n",
    "        raise ValueError('If using `weights` as imagenet with `include_top`'\n",
    "                         ' as true, `classes` should be 1000')\n",
    "    # Determine proper input shape\n",
    "    input_shape = _obtain_input_shape(input_shape,\n",
    "                                      default_size=224,\n",
    "                                      min_size=48,\n",
    "                                      data_format=K.image_data_format(),\n",
    "                                      require_flatten=include_top)\n",
    "\n",
    "    if input_tensor is None:\n",
    "        img_input = Input(shape=input_shape)\n",
    "    else:\n",
    "        if not K.is_keras_tensor(input_tensor):\n",
    "            img_input = Input(tensor=input_tensor, shape=input_shape)\n",
    "        else:\n",
    "            img_input = input_tensor\n",
    "\n",
    "    x = Convolution2D(64, kernel_size=(3, 3), strides=(2, 2), padding=\"same\", activation=\"relu\", name='conv1')(img_input)\n",
    "    x = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), name='maxpool1', padding=\"valid\")(x)\n",
    "\n",
    "    x = _fire(x, (16, 64, 64), name=\"fire2\")\n",
    "    x = _fire(x, (16, 64, 64), name=\"fire3\")\n",
    "\n",
    "    x = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), name='maxpool3', padding=\"valid\")(x)\n",
    "\n",
    "    x = _fire(x, (32, 128, 128), name=\"fire4\")\n",
    "    x = _fire(x, (32, 128, 128), name=\"fire5\")\n",
    "\n",
    "    x = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), name='maxpool5', padding=\"valid\")(x)\n",
    "\n",
    "    x = _fire(x, (48, 192, 192), name=\"fire6\")\n",
    "    x = _fire(x, (48, 192, 192), name=\"fire7\")\n",
    "\n",
    "    x = _fire(x, (64, 256, 256), name=\"fire8\")\n",
    "    x = _fire(x, (64, 256, 256), name=\"fire9\")\n",
    "\n",
    "    if include_top:\n",
    "        x = Dropout(0.5, name='dropout9')(x)\n",
    "\n",
    "        x = Convolution2D(classes, (1, 1), padding='valid', name='conv10')(x)\n",
    "        x = AveragePooling2D(pool_size=(13, 13), name='avgpool10')(x)\n",
    "        x = Flatten(name='flatten10')(x)\n",
    "        x = Activation(\"softmax\", name='softmax')(x)\n",
    "    else:\n",
    "        if pooling == \"avg\":\n",
    "            x = GlobalAveragePooling2D(name=\"avgpool10\")(x)\n",
    "        else:\n",
    "            x = GlobalMaxPooling2D(name=\"maxpool10\")(x)\n",
    "\n",
    "    model = Model(img_input, x, name=\"squeezenet\")\n",
    "\n",
    "    if weights == 'imagenet':\n",
    "        weights_path = get_file('squeezenet_weights.h5',\n",
    "                                WEIGHTS_PATH,\n",
    "                                cache_subdir='models')\n",
    "\n",
    "        model.load_weights(weights_path, )\n",
    "\n",
    "    return model\n",
    "\n",
    "# read the CSV into memory\n",
    "prices = []\n",
    "image_paths = []\n",
    "\n",
    "data_path = \"../datasets/bikes_im/\"\n",
    "with open(\"../datasets/bikes_classified.csv\") as file:\n",
    "    reader = csv.reader(file)\n",
    "    i = -1\n",
    "    for row in reader:\n",
    "        i += 1\n",
    "        index = row[0]\n",
    "        name = row[1]\n",
    "        msrp = row[2]\n",
    "        label = row[3]\n",
    "        \n",
    "        image_path = data_path + index + '.jpg'\n",
    "        image_paths.append(image_path)\n",
    "        prices.append(str(label))\n",
    "        \n",
    "train_indices = np.load(\"bikes_train_indices.npy\")\n",
    "test_indices = np.load(\"bikes_test_indices.npy\")\n",
    "print(train_indices.shape)\n",
    "print(test_indices.shape)\n",
    "\n",
    "def image_generator(indices, batch_size):\n",
    "\n",
    "    num_batches = int(len(indices) / batch_size)\n",
    "    \n",
    "    while True:\n",
    "        for batch_i in range(num_batches):\n",
    "            if batch_i == num_batches - 1:\n",
    "                # special case: return as many as possible\n",
    "                start_i = batch_i * batch_size\n",
    "                batch_indices = indices[start_i:]\n",
    "                \n",
    "                X = np.zeros((len(batch_indices), 224, 224, 3))\n",
    "                Y = np.zeros((len(batch_indices), 4)) # Change to one-hot\n",
    "            \n",
    "            else:\n",
    "                start_i = batch_i * batch_size\n",
    "                end_i = start_i + batch_size\n",
    "\n",
    "                batch_indices = indices[start_i:end_i]\n",
    "\n",
    "                X = np.zeros((batch_size, 224, 224, 3))\n",
    "                Y = np.zeros((batch_size, 4)) # Change to one-hot\n",
    "            \n",
    "            for i, index in enumerate(batch_indices):\n",
    "                img = image.load_img(image_paths[index], target_size=(224, 224))\n",
    "                X[i, :, :, :] = image.img_to_array(img)\n",
    "                # Convert to 1 hot vector\n",
    "                p = prices[index]\n",
    "                if p == \"25\":\n",
    "                    Y[i,:] = np.array([1,0,0,0])\n",
    "                if p == \"50\":\n",
    "                    Y[i,:] = np.array([0,1,0,0])\n",
    "                if p == \"75\":\n",
    "                    Y[i,:] = np.array([0,0,1,0])\n",
    "                if p == \"100\":\n",
    "                    Y[i,:] = np.array([0,0,0,1])\n",
    "            \n",
    "            # use vgg16 preprocessing\n",
    "            X = preprocess_input(X)\n",
    "            \n",
    "            yield (X, Y)\n",
    "\n",
    "# Hyperparameters\n",
    "\n",
    "num_settings = 1\n",
    "\n",
    "hp_dropout = [0.5] * num_settings\n",
    "\n",
    "#RMSprop\n",
    "hp_lr = [0.005] * num_settings\n",
    "hp_rho = [0.9] * num_settings\n",
    "hp_epsilon = [1e-07] * num_settings\n",
    "hp_decay = [0.0] * num_settings\n",
    "\n",
    "# Number of hidden units\n",
    "hp_hidden = [256] * num_settings\n",
    "\n",
    "# Minibatch size\n",
    "hp_mbsize = [64] * num_settings\n",
    "\n",
    "num_epochs = 300\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 224, 224, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                  (None, 112, 112, 64) 1792        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "maxpool1 (MaxPooling2D)         (None, 55, 55, 64)   0           conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "fire2squeeze1x1 (Conv2D)        (None, 55, 55, 16)   1040        maxpool1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "fire2expand1x1 (Conv2D)         (None, 55, 55, 64)   1088        fire2squeeze1x1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire2expand3x3 (Conv2D)         (None, 55, 55, 64)   9280        fire2squeeze1x1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire2 (Concatenate)             (None, 55, 55, 128)  0           fire2expand1x1[0][0]             \n",
      "                                                                 fire2expand3x3[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "fire3squeeze1x1 (Conv2D)        (None, 55, 55, 16)   2064        fire2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "fire3expand1x1 (Conv2D)         (None, 55, 55, 64)   1088        fire3squeeze1x1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire3expand3x3 (Conv2D)         (None, 55, 55, 64)   9280        fire3squeeze1x1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire3 (Concatenate)             (None, 55, 55, 128)  0           fire3expand1x1[0][0]             \n",
      "                                                                 fire3expand3x3[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "maxpool3 (MaxPooling2D)         (None, 27, 27, 128)  0           fire3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "fire4squeeze1x1 (Conv2D)        (None, 27, 27, 32)   4128        maxpool3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "fire4expand1x1 (Conv2D)         (None, 27, 27, 128)  4224        fire4squeeze1x1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire4expand3x3 (Conv2D)         (None, 27, 27, 128)  36992       fire4squeeze1x1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire4 (Concatenate)             (None, 27, 27, 256)  0           fire4expand1x1[0][0]             \n",
      "                                                                 fire4expand3x3[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "fire5squeeze1x1 (Conv2D)        (None, 27, 27, 32)   8224        fire4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "fire5expand1x1 (Conv2D)         (None, 27, 27, 128)  4224        fire5squeeze1x1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire5expand3x3 (Conv2D)         (None, 27, 27, 128)  36992       fire5squeeze1x1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire5 (Concatenate)             (None, 27, 27, 256)  0           fire5expand1x1[0][0]             \n",
      "                                                                 fire5expand3x3[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "maxpool5 (MaxPooling2D)         (None, 13, 13, 256)  0           fire5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "fire6squeeze1x1 (Conv2D)        (None, 13, 13, 48)   12336       maxpool5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "fire6expand1x1 (Conv2D)         (None, 13, 13, 192)  9408        fire6squeeze1x1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire6expand3x3 (Conv2D)         (None, 13, 13, 192)  83136       fire6squeeze1x1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire6 (Concatenate)             (None, 13, 13, 384)  0           fire6expand1x1[0][0]             \n",
      "                                                                 fire6expand3x3[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "fire7squeeze1x1 (Conv2D)        (None, 13, 13, 48)   18480       fire6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "fire7expand1x1 (Conv2D)         (None, 13, 13, 192)  9408        fire7squeeze1x1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire7expand3x3 (Conv2D)         (None, 13, 13, 192)  83136       fire7squeeze1x1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire7 (Concatenate)             (None, 13, 13, 384)  0           fire7expand1x1[0][0]             \n",
      "                                                                 fire7expand3x3[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "fire8squeeze1x1 (Conv2D)        (None, 13, 13, 64)   24640       fire7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "fire8expand1x1 (Conv2D)         (None, 13, 13, 256)  16640       fire8squeeze1x1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire8expand3x3 (Conv2D)         (None, 13, 13, 256)  147712      fire8squeeze1x1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire8 (Concatenate)             (None, 13, 13, 512)  0           fire8expand1x1[0][0]             \n",
      "                                                                 fire8expand3x3[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "fire9squeeze1x1 (Conv2D)        (None, 13, 13, 64)   32832       fire8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "fire9expand1x1 (Conv2D)         (None, 13, 13, 256)  16640       fire9squeeze1x1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire9expand3x3 (Conv2D)         (None, 13, 13, 256)  147712      fire9squeeze1x1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire9 (Concatenate)             (None, 13, 13, 512)  0           fire9expand1x1[0][0]             \n",
      "                                                                 fire9expand3x3[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "sequential_1 (Sequential)       (None, 4)            135428      fire9[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 857,924\n",
      "Trainable params: 857,924\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Richard\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:56: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "C:\\Users\\Richard\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:56: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<generator..., validation_data=<generator..., steps_per_epoch=308, validation_steps=35, callbacks=[<keras.ca..., epochs=300)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "308/308 [==============================] - 76s 246ms/step - loss: 1.1704 - acc: 0.5242 - val_loss: 0.8462 - val_acc: 0.6354\n",
      "Epoch 2/300\n",
      "308/308 [==============================] - 61s 197ms/step - loss: 0.7604 - acc: 0.6550 - val_loss: 0.8028 - val_acc: 0.6701\n",
      "Epoch 3/300\n",
      "308/308 [==============================] - 60s 193ms/step - loss: 0.6558 - acc: 0.7138 - val_loss: 0.6682 - val_acc: 0.7043\n",
      "Epoch 4/300\n",
      "308/308 [==============================] - 58s 187ms/step - loss: 0.5784 - acc: 0.7494 - val_loss: 0.6243 - val_acc: 0.7421\n",
      "Epoch 5/300\n",
      "308/308 [==============================] - 56s 181ms/step - loss: 0.5162 - acc: 0.7829 - val_loss: 0.5835 - val_acc: 0.7612\n",
      "Epoch 6/300\n",
      "308/308 [==============================] - 56s 180ms/step - loss: 0.4549 - acc: 0.8129 - val_loss: 0.6275 - val_acc: 0.7617\n",
      "Epoch 7/300\n",
      "308/308 [==============================] - 55s 179ms/step - loss: 0.4065 - acc: 0.8326 - val_loss: 0.6716 - val_acc: 0.7554\n",
      "Epoch 8/300\n",
      "308/308 [==============================] - 55s 179ms/step - loss: 0.3580 - acc: 0.8559 - val_loss: 0.5834 - val_acc: 0.7781\n",
      "Epoch 9/300\n",
      "308/308 [==============================] - 55s 178ms/step - loss: 0.3137 - acc: 0.8740 - val_loss: 0.7453 - val_acc: 0.7706\n",
      "Epoch 10/300\n",
      "308/308 [==============================] - 54s 176ms/step - loss: 0.2796 - acc: 0.8898 - val_loss: 0.6818 - val_acc: 0.7768\n",
      "Epoch 11/300\n",
      "308/308 [==============================] - 55s 177ms/step - loss: 0.2467 - acc: 0.9041 - val_loss: 0.7692 - val_acc: 0.7710\n",
      "Epoch 12/300\n",
      "308/308 [==============================] - 55s 179ms/step - loss: 0.2181 - acc: 0.9169 - val_loss: 0.6929 - val_acc: 0.7875\n",
      "Epoch 13/300\n",
      "308/308 [==============================] - 55s 179ms/step - loss: 0.1884 - acc: 0.9275 - val_loss: 0.7298 - val_acc: 0.7808\n",
      "Epoch 14/300\n",
      "308/308 [==============================] - 55s 179ms/step - loss: 0.1730 - acc: 0.9355 - val_loss: 0.6992 - val_acc: 0.7817\n",
      "Epoch 15/300\n",
      "308/308 [==============================] - 55s 179ms/step - loss: 0.1499 - acc: 0.9426 - val_loss: 0.8177 - val_acc: 0.7861\n",
      "Epoch 16/300\n",
      "308/308 [==============================] - 55s 178ms/step - loss: 0.1402 - acc: 0.9470 - val_loss: 0.8174 - val_acc: 0.7932\n",
      "Epoch 17/300\n",
      "308/308 [==============================] - 55s 179ms/step - loss: 0.1312 - acc: 0.9524 - val_loss: 0.8493 - val_acc: 0.7826\n",
      "Epoch 18/300\n",
      "308/308 [==============================] - 55s 179ms/step - loss: 0.1167 - acc: 0.9583 - val_loss: 0.8899 - val_acc: 0.7870\n",
      "Epoch 19/300\n",
      "308/308 [==============================] - 55s 180ms/step - loss: 0.1044 - acc: 0.9617 - val_loss: 1.0396 - val_acc: 0.7661\n",
      "Epoch 20/300\n",
      "308/308 [==============================] - 55s 178ms/step - loss: 0.1008 - acc: 0.9636 - val_loss: 1.0098 - val_acc: 0.7701\n",
      "Epoch 21/300\n",
      "308/308 [==============================] - 55s 178ms/step - loss: 0.0980 - acc: 0.9665 - val_loss: 0.9360 - val_acc: 0.7972\n",
      "Epoch 22/300\n",
      "308/308 [==============================] - 55s 179ms/step - loss: 0.0964 - acc: 0.9670 - val_loss: 1.0702 - val_acc: 0.7688\n",
      "Epoch 23/300\n",
      "308/308 [==============================] - 55s 179ms/step - loss: 0.0882 - acc: 0.9681 - val_loss: 0.9396 - val_acc: 0.7852\n",
      "Epoch 24/300\n",
      "308/308 [==============================] - 55s 178ms/step - loss: 0.0855 - acc: 0.9704 - val_loss: 0.9399 - val_acc: 0.7919\n",
      "Epoch 25/300\n",
      "308/308 [==============================] - 55s 179ms/step - loss: 0.0877 - acc: 0.9699 - val_loss: 0.8631 - val_acc: 0.7981\n",
      "Epoch 26/300\n",
      "308/308 [==============================] - 55s 179ms/step - loss: 0.0810 - acc: 0.9711 - val_loss: 0.9580 - val_acc: 0.8008\n",
      "Epoch 27/300\n",
      "308/308 [==============================] - 55s 180ms/step - loss: 0.0796 - acc: 0.9732 - val_loss: 1.0002 - val_acc: 0.7950\n",
      "Epoch 28/300\n",
      "308/308 [==============================] - 56s 180ms/step - loss: 0.0790 - acc: 0.9735 - val_loss: 1.0848 - val_acc: 0.8012\n",
      "Epoch 29/300\n",
      "308/308 [==============================] - 55s 178ms/step - loss: 0.0724 - acc: 0.9742 - val_loss: 0.9699 - val_acc: 0.8044\n",
      "Epoch 30/300\n",
      "308/308 [==============================] - 55s 179ms/step - loss: 0.0789 - acc: 0.9736 - val_loss: 0.9758 - val_acc: 0.7999\n",
      "Epoch 31/300\n",
      "308/308 [==============================] - 55s 179ms/step - loss: 0.0662 - acc: 0.9779 - val_loss: 0.9877 - val_acc: 0.7999\n",
      "Epoch 32/300\n",
      "308/308 [==============================] - 55s 179ms/step - loss: 0.0671 - acc: 0.9767 - val_loss: 0.9175 - val_acc: 0.7950\n",
      "Epoch 33/300\n",
      "308/308 [==============================] - 54s 177ms/step - loss: 0.0669 - acc: 0.9776 - val_loss: 1.1210 - val_acc: 0.7875\n",
      "Epoch 34/300\n",
      "308/308 [==============================] - 55s 180ms/step - loss: 0.0704 - acc: 0.9755 - val_loss: 1.1552 - val_acc: 0.7710\n",
      "Epoch 35/300\n",
      "308/308 [==============================] - 54s 177ms/step - loss: 0.0622 - acc: 0.9786 - val_loss: 1.0473 - val_acc: 0.8128\n",
      "Epoch 36/300\n",
      "308/308 [==============================] - 55s 179ms/step - loss: 0.0638 - acc: 0.9783 - val_loss: 1.0702 - val_acc: 0.8097\n",
      "Epoch 37/300\n",
      "308/308 [==============================] - 55s 179ms/step - loss: 0.0624 - acc: 0.9787 - val_loss: 1.0059 - val_acc: 0.8044\n",
      "Epoch 38/300\n",
      "308/308 [==============================] - 55s 178ms/step - loss: 0.0638 - acc: 0.9784 - val_loss: 1.0942 - val_acc: 0.8030\n",
      "Epoch 39/300\n",
      "308/308 [==============================] - 55s 179ms/step - loss: 0.0624 - acc: 0.9802 - val_loss: 1.1539 - val_acc: 0.8039\n",
      "Epoch 40/300\n",
      "308/308 [==============================] - 55s 179ms/step - loss: 0.0616 - acc: 0.9801 - val_loss: 1.1529 - val_acc: 0.8128\n",
      "Epoch 41/300\n",
      "308/308 [==============================] - 55s 179ms/step - loss: 0.0578 - acc: 0.9803 - val_loss: 1.1646 - val_acc: 0.8057\n",
      "Epoch 42/300\n",
      "308/308 [==============================] - 55s 179ms/step - loss: 0.0597 - acc: 0.9809 - val_loss: 1.1680 - val_acc: 0.8070\n",
      "Epoch 43/300\n",
      "308/308 [==============================] - 55s 179ms/step - loss: 0.0592 - acc: 0.9799 - val_loss: 1.1852 - val_acc: 0.7981\n",
      "Epoch 44/300\n",
      "308/308 [==============================] - 56s 181ms/step - loss: 0.0590 - acc: 0.9809 - val_loss: 1.1556 - val_acc: 0.8057\n",
      "Epoch 45/300\n",
      "308/308 [==============================] - 55s 179ms/step - loss: 0.0558 - acc: 0.9812 - val_loss: 1.1408 - val_acc: 0.8026\n",
      "Epoch 46/300\n",
      "308/308 [==============================] - 55s 178ms/step - loss: 0.0551 - acc: 0.9814 - val_loss: 1.2063 - val_acc: 0.8079\n",
      "Epoch 47/300\n",
      "308/308 [==============================] - 55s 177ms/step - loss: 0.0553 - acc: 0.9812 - val_loss: 1.3886 - val_acc: 0.7875\n",
      "Epoch 48/300\n",
      "308/308 [==============================] - 55s 179ms/step - loss: 0.0540 - acc: 0.9820 - val_loss: 1.3214 - val_acc: 0.7977\n",
      "Epoch 49/300\n",
      "308/308 [==============================] - 55s 178ms/step - loss: 0.0588 - acc: 0.9808 - val_loss: 1.1473 - val_acc: 0.8026\n",
      "Epoch 50/300\n",
      "308/308 [==============================] - 56s 180ms/step - loss: 0.0577 - acc: 0.9815 - val_loss: 1.1980 - val_acc: 0.8155\n",
      "Epoch 51/300\n",
      "308/308 [==============================] - 55s 180ms/step - loss: 0.0538 - acc: 0.9821 - val_loss: 1.2766 - val_acc: 0.8021\n",
      "Epoch 52/300\n",
      "308/308 [==============================] - 55s 179ms/step - loss: 0.0527 - acc: 0.9833 - val_loss: 1.2756 - val_acc: 0.7990\n",
      "Epoch 53/300\n",
      "308/308 [==============================] - 55s 179ms/step - loss: 0.0544 - acc: 0.9819 - val_loss: 1.2726 - val_acc: 0.7946\n",
      "Epoch 54/300\n",
      "308/308 [==============================] - 55s 180ms/step - loss: 0.0523 - acc: 0.9827 - val_loss: 1.2533 - val_acc: 0.7924\n",
      "Epoch 55/300\n",
      "308/308 [==============================] - 55s 179ms/step - loss: 0.0558 - acc: 0.9823 - val_loss: 1.1280 - val_acc: 0.7955\n",
      "Epoch 56/300\n",
      "308/308 [==============================] - 55s 178ms/step - loss: 0.0506 - acc: 0.9835 - val_loss: 1.5048 - val_acc: 0.7839\n",
      "Epoch 57/300\n",
      "308/308 [==============================] - 55s 178ms/step - loss: 0.0534 - acc: 0.9829 - val_loss: 1.1711 - val_acc: 0.8061\n",
      "Epoch 58/300\n",
      "308/308 [==============================] - 55s 179ms/step - loss: 0.0526 - acc: 0.9835 - val_loss: 1.2872 - val_acc: 0.8133\n",
      "Epoch 59/300\n",
      "308/308 [==============================] - 55s 179ms/step - loss: 0.0508 - acc: 0.9842 - val_loss: 1.2208 - val_acc: 0.7941\n",
      "Epoch 60/300\n",
      "308/308 [==============================] - 55s 178ms/step - loss: 0.0467 - acc: 0.9827 - val_loss: 1.2594 - val_acc: 0.8057\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/300\n",
      "308/308 [==============================] - 55s 177ms/step - loss: 0.0517 - acc: 0.9833 - val_loss: 1.2238 - val_acc: 0.8061\n",
      "Epoch 62/300\n",
      "308/308 [==============================] - 54s 176ms/step - loss: 0.0496 - acc: 0.9835 - val_loss: 1.1062 - val_acc: 0.7977\n",
      "Epoch 63/300\n",
      "308/308 [==============================] - 54s 176ms/step - loss: 0.0503 - acc: 0.9837 - val_loss: 1.1836 - val_acc: 0.8070\n",
      "Epoch 64/300\n",
      "308/308 [==============================] - 54s 176ms/step - loss: 0.0515 - acc: 0.9841 - val_loss: 1.2565 - val_acc: 0.8092\n",
      "Epoch 65/300\n",
      "308/308 [==============================] - 55s 178ms/step - loss: 0.0559 - acc: 0.9810 - val_loss: 1.2482 - val_acc: 0.7937\n",
      "Epoch 66/300\n",
      "308/308 [==============================] - 55s 177ms/step - loss: 0.0511 - acc: 0.9842 - val_loss: 1.2448 - val_acc: 0.7946\n",
      "Epoch 67/300\n",
      "308/308 [==============================] - 54s 176ms/step - loss: 0.0531 - acc: 0.9824 - val_loss: 1.3250 - val_acc: 0.8004\n",
      "Epoch 68/300\n",
      "308/308 [==============================] - 54s 176ms/step - loss: 0.0515 - acc: 0.9838 - val_loss: 1.3738 - val_acc: 0.7945\n",
      "Epoch 69/300\n",
      "308/308 [==============================] - 54s 177ms/step - loss: 0.0539 - acc: 0.9835 - val_loss: 1.2255 - val_acc: 0.7999\n",
      "Epoch 70/300\n",
      "308/308 [==============================] - 55s 178ms/step - loss: 0.0497 - acc: 0.9839 - val_loss: 1.3424 - val_acc: 0.8004\n",
      "Epoch 71/300\n",
      "308/308 [==============================] - 54s 177ms/step - loss: 0.0507 - acc: 0.9825 - val_loss: 1.2291 - val_acc: 0.8021\n",
      "Epoch 72/300\n",
      "308/308 [==============================] - 55s 177ms/step - loss: 0.0499 - acc: 0.9841 - val_loss: 1.2070 - val_acc: 0.7995\n",
      "Epoch 73/300\n",
      "308/308 [==============================] - 54s 176ms/step - loss: 0.0477 - acc: 0.9844 - val_loss: 1.3722 - val_acc: 0.8168\n",
      "Epoch 74/300\n",
      "308/308 [==============================] - 55s 178ms/step - loss: 0.0475 - acc: 0.9848 - val_loss: 1.3397 - val_acc: 0.7950\n",
      "Epoch 75/300\n",
      "308/308 [==============================] - 55s 177ms/step - loss: 0.0568 - acc: 0.9833 - val_loss: 1.3216 - val_acc: 0.7972\n",
      "Epoch 76/300\n",
      "308/308 [==============================] - 55s 177ms/step - loss: 0.0455 - acc: 0.9858 - val_loss: 1.5042 - val_acc: 0.7906\n",
      "Epoch 77/300\n",
      "308/308 [==============================] - 55s 179ms/step - loss: 0.0476 - acc: 0.9846 - val_loss: 1.4228 - val_acc: 0.7928\n",
      "Epoch 78/300\n",
      "308/308 [==============================] - 55s 177ms/step - loss: 0.0526 - acc: 0.9831 - val_loss: 1.2719 - val_acc: 0.8061\n",
      "Epoch 79/300\n",
      "308/308 [==============================] - 55s 178ms/step - loss: 0.0523 - acc: 0.9835 - val_loss: 1.3866 - val_acc: 0.8092\n",
      "Epoch 80/300\n",
      "308/308 [==============================] - 55s 178ms/step - loss: 0.0469 - acc: 0.9850 - val_loss: 1.4783 - val_acc: 0.7719\n",
      "Epoch 81/300\n",
      "308/308 [==============================] - 55s 177ms/step - loss: 0.0482 - acc: 0.9842 - val_loss: 1.4747 - val_acc: 0.8061\n",
      "Epoch 82/300\n",
      "308/308 [==============================] - 54s 176ms/step - loss: 0.0448 - acc: 0.9853 - val_loss: 1.4242 - val_acc: 0.8008\n",
      "Epoch 83/300\n",
      "308/308 [==============================] - 55s 177ms/step - loss: 0.0485 - acc: 0.9852 - val_loss: 1.4453 - val_acc: 0.8035\n",
      "Epoch 84/300\n",
      "308/308 [==============================] - 55s 177ms/step - loss: 0.0498 - acc: 0.9838 - val_loss: 1.4770 - val_acc: 0.7937\n",
      "Epoch 85/300\n",
      "308/308 [==============================] - 54s 177ms/step - loss: 0.0474 - acc: 0.9852 - val_loss: 1.5250 - val_acc: 0.7915\n",
      "Epoch 86/300\n",
      "308/308 [==============================] - 54s 177ms/step - loss: 0.0467 - acc: 0.9852 - val_loss: 1.4754 - val_acc: 0.7910\n",
      "Epoch 87/300\n",
      "308/308 [==============================] - 55s 178ms/step - loss: 0.0488 - acc: 0.9849 - val_loss: 1.5643 - val_acc: 0.7892\n",
      "Epoch 88/300\n",
      "308/308 [==============================] - 54s 177ms/step - loss: 0.0495 - acc: 0.9845 - val_loss: 1.2080 - val_acc: 0.7946\n",
      "Epoch 89/300\n",
      "308/308 [==============================] - 55s 177ms/step - loss: 0.0452 - acc: 0.9847 - val_loss: 1.4338 - val_acc: 0.7955\n",
      "Epoch 90/300\n",
      "308/308 [==============================] - 55s 178ms/step - loss: 0.0469 - acc: 0.9844 - val_loss: 1.4907 - val_acc: 0.7941\n",
      "Epoch 91/300\n",
      "308/308 [==============================] - 54s 177ms/step - loss: 0.0484 - acc: 0.9859 - val_loss: 1.4458 - val_acc: 0.7999\n",
      "Epoch 92/300\n",
      "308/308 [==============================] - 54s 176ms/step - loss: 0.0439 - acc: 0.9859 - val_loss: 1.5822 - val_acc: 0.8124\n",
      "Epoch 93/300\n",
      "308/308 [==============================] - 54s 176ms/step - loss: 0.0481 - acc: 0.9846 - val_loss: 1.4557 - val_acc: 0.8044\n",
      "Epoch 94/300\n",
      "308/308 [==============================] - 54s 176ms/step - loss: 0.0454 - acc: 0.9847 - val_loss: 1.2661 - val_acc: 0.8017\n",
      "Epoch 95/300\n",
      "308/308 [==============================] - 54s 176ms/step - loss: 0.0431 - acc: 0.9858 - val_loss: 1.4188 - val_acc: 0.8039\n",
      "Epoch 96/300\n",
      "308/308 [==============================] - 54s 177ms/step - loss: 0.0455 - acc: 0.9860 - val_loss: 1.5018 - val_acc: 0.7959\n",
      "Epoch 97/300\n",
      "308/308 [==============================] - 54s 177ms/step - loss: 0.0442 - acc: 0.9850 - val_loss: 1.4749 - val_acc: 0.8057\n",
      "Epoch 98/300\n",
      "308/308 [==============================] - 55s 180ms/step - loss: 0.0482 - acc: 0.9839 - val_loss: 1.1986 - val_acc: 0.8061\n",
      "Epoch 99/300\n",
      "308/308 [==============================] - 55s 180ms/step - loss: 0.0466 - acc: 0.9859 - val_loss: 1.3320 - val_acc: 0.8035\n",
      "Epoch 100/300\n",
      "308/308 [==============================] - 55s 179ms/step - loss: 0.0471 - acc: 0.9851 - val_loss: 1.3338 - val_acc: 0.8092\n",
      "Epoch 101/300\n",
      "308/308 [==============================] - 55s 179ms/step - loss: 0.0508 - acc: 0.9846 - val_loss: 1.4881 - val_acc: 0.8035\n",
      "Epoch 102/300\n",
      "308/308 [==============================] - 55s 179ms/step - loss: 0.0447 - acc: 0.9862 - val_loss: 1.2682 - val_acc: 0.8100\n",
      "Epoch 103/300\n",
      "308/308 [==============================] - 54s 177ms/step - loss: 0.0488 - acc: 0.9850 - val_loss: 1.2792 - val_acc: 0.7955\n",
      "Epoch 104/300\n",
      "308/308 [==============================] - 55s 177ms/step - loss: 0.0444 - acc: 0.9862 - val_loss: 1.2605 - val_acc: 0.8110\n",
      "Epoch 105/300\n",
      "308/308 [==============================] - 55s 177ms/step - loss: 0.0492 - acc: 0.9848 - val_loss: 1.4202 - val_acc: 0.7932\n",
      "Epoch 106/300\n",
      "308/308 [==============================] - 55s 179ms/step - loss: 0.0484 - acc: 0.9860 - val_loss: 1.3337 - val_acc: 0.8075\n",
      "Epoch 107/300\n",
      "308/308 [==============================] - 55s 179ms/step - loss: 0.0437 - acc: 0.9864 - val_loss: 1.2884 - val_acc: 0.8133\n",
      "Epoch 108/300\n",
      "308/308 [==============================] - 55s 179ms/step - loss: 0.0461 - acc: 0.9864 - val_loss: 1.3807 - val_acc: 0.8048\n",
      "Epoch 109/300\n",
      "308/308 [==============================] - 55s 179ms/step - loss: 0.0456 - acc: 0.9864 - val_loss: 1.3943 - val_acc: 0.8119\n",
      "Epoch 110/300\n",
      "308/308 [==============================] - 55s 179ms/step - loss: 0.0458 - acc: 0.9860 - val_loss: 1.2372 - val_acc: 0.8039\n",
      "Epoch 111/300\n",
      "308/308 [==============================] - 55s 178ms/step - loss: 0.0474 - acc: 0.9857 - val_loss: 1.4772 - val_acc: 0.8030\n",
      "Epoch 112/300\n",
      "308/308 [==============================] - 55s 180ms/step - loss: 0.0433 - acc: 0.9859 - val_loss: 1.4749 - val_acc: 0.8128\n",
      "Epoch 113/300\n",
      "308/308 [==============================] - 55s 179ms/step - loss: 0.0454 - acc: 0.9867 - val_loss: 1.4761 - val_acc: 0.8039\n",
      "Epoch 114/300\n",
      "308/308 [==============================] - 55s 178ms/step - loss: 0.0451 - acc: 0.9861 - val_loss: 1.4587 - val_acc: 0.8079\n",
      "Epoch 115/300\n",
      "308/308 [==============================] - 55s 178ms/step - loss: 0.0454 - acc: 0.9862 - val_loss: 1.4881 - val_acc: 0.8057\n",
      "Epoch 116/300\n",
      "308/308 [==============================] - 55s 178ms/step - loss: 0.0479 - acc: 0.9854 - val_loss: 1.4986 - val_acc: 0.8173\n",
      "Epoch 117/300\n",
      "308/308 [==============================] - 55s 178ms/step - loss: 0.0475 - acc: 0.9853 - val_loss: 1.3618 - val_acc: 0.7946\n",
      "Epoch 118/300\n",
      "308/308 [==============================] - 55s 179ms/step - loss: 0.0457 - acc: 0.9858 - val_loss: 1.5314 - val_acc: 0.8052\n",
      "Epoch 119/300\n",
      "308/308 [==============================] - 55s 177ms/step - loss: 0.0457 - acc: 0.9863 - val_loss: 1.5988 - val_acc: 0.8030\n",
      "Epoch 120/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "308/308 [==============================] - 55s 179ms/step - loss: 0.0459 - acc: 0.9862 - val_loss: 1.7914 - val_acc: 0.7977\n",
      "Epoch 121/300\n",
      "308/308 [==============================] - 55s 180ms/step - loss: 0.0449 - acc: 0.9863 - val_loss: 1.4812 - val_acc: 0.8070\n",
      "Epoch 122/300\n",
      "308/308 [==============================] - 55s 177ms/step - loss: 0.0410 - acc: 0.9873 - val_loss: 1.7461 - val_acc: 0.8052\n",
      "Epoch 123/300\n",
      "308/308 [==============================] - 55s 178ms/step - loss: 0.0409 - acc: 0.9876 - val_loss: 1.3085 - val_acc: 0.8066\n",
      "Epoch 124/300\n",
      "308/308 [==============================] - 55s 179ms/step - loss: 0.0434 - acc: 0.9853 - val_loss: 1.5879 - val_acc: 0.8026\n",
      "Epoch 125/300\n",
      "308/308 [==============================] - 55s 178ms/step - loss: 0.0468 - acc: 0.9853 - val_loss: 1.5368 - val_acc: 0.8004\n",
      "Epoch 126/300\n",
      "308/308 [==============================] - 55s 178ms/step - loss: 0.0408 - acc: 0.9872 - val_loss: 1.4921 - val_acc: 0.7990\n",
      "Epoch 127/300\n",
      "308/308 [==============================] - 55s 179ms/step - loss: 0.0477 - acc: 0.9858 - val_loss: 1.4139 - val_acc: 0.7995\n",
      "Epoch 128/300\n",
      "308/308 [==============================] - 55s 179ms/step - loss: 0.0447 - acc: 0.9869 - val_loss: 1.4891 - val_acc: 0.8012\n",
      "Epoch 129/300\n",
      "308/308 [==============================] - 55s 179ms/step - loss: 0.0447 - acc: 0.9863 - val_loss: 1.2997 - val_acc: 0.8088\n",
      "Epoch 130/300\n",
      "308/308 [==============================] - 55s 178ms/step - loss: 0.0439 - acc: 0.9868 - val_loss: 1.4352 - val_acc: 0.8150\n",
      "Epoch 131/300\n",
      "308/308 [==============================] - 55s 177ms/step - loss: 0.0391 - acc: 0.9871 - val_loss: 1.5354 - val_acc: 0.8092\n",
      "Epoch 132/300\n",
      "308/308 [==============================] - 55s 178ms/step - loss: 0.0434 - acc: 0.9872 - val_loss: 1.6985 - val_acc: 0.7986\n",
      "Epoch 133/300\n",
      "308/308 [==============================] - 55s 178ms/step - loss: 0.0429 - acc: 0.9860 - val_loss: 1.5380 - val_acc: 0.8039\n",
      "Epoch 134/300\n",
      "308/308 [==============================] - 55s 177ms/step - loss: 0.0446 - acc: 0.9863 - val_loss: 1.5525 - val_acc: 0.8066\n",
      "Epoch 135/300\n",
      "308/308 [==============================] - 55s 179ms/step - loss: 0.0508 - acc: 0.9860 - val_loss: 1.4479 - val_acc: 0.8101\n",
      "Epoch 136/300\n",
      "308/308 [==============================] - 55s 177ms/step - loss: 0.0405 - acc: 0.9875 - val_loss: 1.6576 - val_acc: 0.7989\n",
      "Epoch 137/300\n",
      "308/308 [==============================] - 55s 177ms/step - loss: 0.0458 - acc: 0.9866 - val_loss: 1.4430 - val_acc: 0.7968\n",
      "Epoch 138/300\n",
      "308/308 [==============================] - 55s 179ms/step - loss: 0.0448 - acc: 0.9863 - val_loss: 1.7497 - val_acc: 0.7964\n",
      "Epoch 139/300\n",
      "308/308 [==============================] - 55s 178ms/step - loss: 0.0442 - acc: 0.9863 - val_loss: 1.6078 - val_acc: 0.8057\n",
      "Epoch 140/300\n",
      "308/308 [==============================] - 55s 178ms/step - loss: 0.0443 - acc: 0.9873 - val_loss: 1.4562 - val_acc: 0.8106\n",
      "Epoch 141/300\n",
      "308/308 [==============================] - 55s 180ms/step - loss: 0.0423 - acc: 0.9874 - val_loss: 1.5383 - val_acc: 0.8021\n",
      "Epoch 142/300\n",
      "308/308 [==============================] - 55s 180ms/step - loss: 0.0409 - acc: 0.9875 - val_loss: 1.4521 - val_acc: 0.8079\n",
      "Epoch 143/300\n",
      "308/308 [==============================] - 55s 179ms/step - loss: 0.0393 - acc: 0.9876 - val_loss: 1.5504 - val_acc: 0.7901\n",
      "Epoch 144/300\n",
      "308/308 [==============================] - 55s 178ms/step - loss: 0.0454 - acc: 0.9865 - val_loss: 1.4339 - val_acc: 0.8004\n",
      "Epoch 145/300\n",
      "308/308 [==============================] - 55s 178ms/step - loss: 0.0401 - acc: 0.9869 - val_loss: 1.4848 - val_acc: 0.8173\n",
      "Epoch 146/300\n",
      "308/308 [==============================] - 55s 178ms/step - loss: 0.0376 - acc: 0.9876 - val_loss: 1.6525 - val_acc: 0.8159\n",
      "Epoch 147/300\n",
      "308/308 [==============================] - 55s 178ms/step - loss: 0.0433 - acc: 0.9859 - val_loss: 1.5161 - val_acc: 0.8084\n",
      "Epoch 148/300\n",
      "308/308 [==============================] - 55s 178ms/step - loss: 0.0463 - acc: 0.9861 - val_loss: 1.6355 - val_acc: 0.8052\n",
      "Epoch 149/300\n",
      "308/308 [==============================] - 55s 178ms/step - loss: 0.0404 - acc: 0.9877 - val_loss: 1.4401 - val_acc: 0.8110\n",
      "Epoch 150/300\n",
      "308/308 [==============================] - 55s 177ms/step - loss: 0.0452 - acc: 0.9864 - val_loss: 1.4145 - val_acc: 0.8079\n",
      "Epoch 151/300\n",
      "308/308 [==============================] - 55s 179ms/step - loss: 0.0429 - acc: 0.9868 - val_loss: 1.5538 - val_acc: 0.8124\n",
      "Epoch 152/300\n",
      "308/308 [==============================] - 55s 179ms/step - loss: 0.0418 - acc: 0.9873 - val_loss: 1.5483 - val_acc: 0.8057\n",
      "Epoch 153/300\n",
      "308/308 [==============================] - 56s 181ms/step - loss: 0.0414 - acc: 0.9869 - val_loss: 1.5738 - val_acc: 0.8092\n",
      "Epoch 154/300\n",
      "308/308 [==============================] - 56s 183ms/step - loss: 0.0454 - acc: 0.9866 - val_loss: 1.7197 - val_acc: 0.8057\n",
      "Epoch 155/300\n",
      "308/308 [==============================] - 55s 177ms/step - loss: 0.0478 - acc: 0.9859 - val_loss: 1.6040 - val_acc: 0.8057\n",
      "Epoch 156/300\n",
      "308/308 [==============================] - 55s 179ms/step - loss: 0.0482 - acc: 0.9864 - val_loss: 1.3490 - val_acc: 0.8044\n",
      "Epoch 157/300\n",
      "308/308 [==============================] - 55s 178ms/step - loss: 0.0416 - acc: 0.9873 - val_loss: 1.6413 - val_acc: 0.7928\n",
      "Epoch 158/300\n",
      "308/308 [==============================] - 55s 178ms/step - loss: 0.0435 - acc: 0.9866 - val_loss: 1.6629 - val_acc: 0.7977\n",
      "Epoch 159/300\n",
      "308/308 [==============================] - 56s 180ms/step - loss: 0.0492 - acc: 0.9867 - val_loss: 1.2707 - val_acc: 0.8052\n",
      "Epoch 160/300\n",
      "308/308 [==============================] - 55s 180ms/step - loss: 0.0436 - acc: 0.9871 - val_loss: 1.7839 - val_acc: 0.7986\n",
      "Epoch 161/300\n",
      "308/308 [==============================] - 55s 180ms/step - loss: 0.0484 - acc: 0.9867 - val_loss: 1.4703 - val_acc: 0.8012\n",
      "Epoch 162/300\n",
      "308/308 [==============================] - 55s 179ms/step - loss: 0.0448 - acc: 0.9865 - val_loss: 1.5309 - val_acc: 0.8066\n",
      "Epoch 163/300\n",
      "308/308 [==============================] - 56s 181ms/step - loss: 0.0384 - acc: 0.9885 - val_loss: 1.6557 - val_acc: 0.8084\n",
      "Epoch 164/300\n",
      "308/308 [==============================] - 55s 180ms/step - loss: 0.0399 - acc: 0.9874 - val_loss: 1.7977 - val_acc: 0.7884\n",
      "Epoch 165/300\n",
      "308/308 [==============================] - 55s 178ms/step - loss: 0.0441 - acc: 0.9874 - val_loss: 1.5296 - val_acc: 0.8101\n",
      "Epoch 166/300\n",
      "308/308 [==============================] - 55s 178ms/step - loss: 0.0452 - acc: 0.9869 - val_loss: 1.5459 - val_acc: 0.8115\n",
      "Epoch 167/300\n",
      "308/308 [==============================] - 55s 179ms/step - loss: 0.0390 - acc: 0.9883 - val_loss: 1.5928 - val_acc: 0.8057\n",
      "Epoch 168/300\n",
      "308/308 [==============================] - 55s 179ms/step - loss: 0.0402 - acc: 0.9875 - val_loss: 1.4607 - val_acc: 0.8021\n",
      "Epoch 169/300\n",
      "308/308 [==============================] - 55s 180ms/step - loss: 0.0423 - acc: 0.9876 - val_loss: 1.6854 - val_acc: 0.8008\n",
      "Epoch 170/300\n",
      "308/308 [==============================] - 56s 181ms/step - loss: 0.0413 - acc: 0.9879 - val_loss: 1.6536 - val_acc: 0.7901\n",
      "Epoch 171/300\n",
      "308/308 [==============================] - 56s 180ms/step - loss: 0.0452 - acc: 0.9871 - val_loss: 1.5784 - val_acc: 0.8052\n",
      "Epoch 172/300\n",
      "308/308 [==============================] - 56s 180ms/step - loss: 0.0397 - acc: 0.9878 - val_loss: 1.5484 - val_acc: 0.8141\n",
      "Epoch 173/300\n",
      "308/308 [==============================] - 55s 179ms/step - loss: 0.0424 - acc: 0.9873 - val_loss: 1.8106 - val_acc: 0.7932\n",
      "Epoch 174/300\n",
      "308/308 [==============================] - 55s 179ms/step - loss: 0.0409 - acc: 0.9880 - val_loss: 1.6586 - val_acc: 0.8141\n",
      "Epoch 175/300\n",
      "308/308 [==============================] - 55s 177ms/step - loss: 0.0450 - acc: 0.9865 - val_loss: 1.5440 - val_acc: 0.8092\n",
      "Epoch 176/300\n",
      "308/308 [==============================] - 55s 179ms/step - loss: 0.0436 - acc: 0.9872 - val_loss: 1.5340 - val_acc: 0.8066\n",
      "Epoch 177/300\n",
      "308/308 [==============================] - 55s 180ms/step - loss: 0.0456 - acc: 0.9872 - val_loss: 1.6397 - val_acc: 0.8008\n",
      "Epoch 178/300\n",
      "308/308 [==============================] - 55s 177ms/step - loss: 0.0377 - acc: 0.9877 - val_loss: 1.6538 - val_acc: 0.7950\n",
      "Epoch 179/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "308/308 [==============================] - 54s 177ms/step - loss: 0.0367 - acc: 0.9880 - val_loss: 1.4150 - val_acc: 0.8048\n",
      "Epoch 180/300\n",
      "308/308 [==============================] - 55s 180ms/step - loss: 0.0486 - acc: 0.9870 - val_loss: 1.5030 - val_acc: 0.8052\n",
      "Epoch 181/300\n",
      "308/308 [==============================] - 55s 178ms/step - loss: 0.0409 - acc: 0.9880 - val_loss: 1.5402 - val_acc: 0.8124\n",
      "Epoch 182/300\n",
      "308/308 [==============================] - 55s 180ms/step - loss: 0.0415 - acc: 0.9875 - val_loss: 1.4877 - val_acc: 0.8128\n",
      "Epoch 183/300\n",
      "308/308 [==============================] - 55s 178ms/step - loss: 0.0370 - acc: 0.9888 - val_loss: 1.5961 - val_acc: 0.7995\n",
      "Epoch 184/300\n",
      "308/308 [==============================] - 55s 179ms/step - loss: 0.0410 - acc: 0.9878 - val_loss: 1.6000 - val_acc: 0.8061\n",
      "Epoch 185/300\n",
      "308/308 [==============================] - 55s 180ms/step - loss: 0.0392 - acc: 0.9881 - val_loss: 1.4762 - val_acc: 0.8092\n",
      "Epoch 186/300\n",
      "308/308 [==============================] - 55s 179ms/step - loss: 0.0405 - acc: 0.9878 - val_loss: 1.8054 - val_acc: 0.7861\n",
      "Epoch 187/300\n",
      "308/308 [==============================] - 55s 180ms/step - loss: 0.0413 - acc: 0.9873 - val_loss: 1.3802 - val_acc: 0.7946\n",
      "Epoch 188/300\n",
      "308/308 [==============================] - 55s 179ms/step - loss: 0.0421 - acc: 0.9881 - val_loss: 1.4876 - val_acc: 0.8097\n",
      "Epoch 189/300\n",
      "308/308 [==============================] - 55s 179ms/step - loss: 0.0413 - acc: 0.9875 - val_loss: 1.4690 - val_acc: 0.8079\n",
      "Epoch 190/300\n",
      "308/308 [==============================] - 55s 178ms/step - loss: 0.0390 - acc: 0.9882 - val_loss: 1.6789 - val_acc: 0.8057\n",
      "Epoch 191/300\n",
      "308/308 [==============================] - 55s 180ms/step - loss: 0.0386 - acc: 0.9882 - val_loss: 1.5163 - val_acc: 0.8008\n",
      "Epoch 192/300\n",
      "308/308 [==============================] - 55s 178ms/step - loss: 0.0381 - acc: 0.9883 - val_loss: 1.8415 - val_acc: 0.8057\n",
      "Epoch 193/300\n",
      "308/308 [==============================] - 55s 178ms/step - loss: 0.0408 - acc: 0.9874 - val_loss: 1.6986 - val_acc: 0.8061\n",
      "Epoch 194/300\n",
      "308/308 [==============================] - 55s 178ms/step - loss: 0.0428 - acc: 0.9880 - val_loss: 1.5331 - val_acc: 0.8026\n",
      "Epoch 195/300\n",
      "308/308 [==============================] - 55s 178ms/step - loss: 0.0412 - acc: 0.9878 - val_loss: 1.5831 - val_acc: 0.8048\n",
      "Epoch 196/300\n",
      "308/308 [==============================] - 55s 178ms/step - loss: 0.0398 - acc: 0.9880 - val_loss: 1.7104 - val_acc: 0.7861\n",
      "Epoch 197/300\n",
      "308/308 [==============================] - 55s 180ms/step - loss: 0.0445 - acc: 0.9876 - val_loss: 1.3932 - val_acc: 0.8110\n",
      "Epoch 198/300\n",
      "308/308 [==============================] - 55s 178ms/step - loss: 0.0409 - acc: 0.9886 - val_loss: 1.5713 - val_acc: 0.8092\n",
      "Epoch 199/300\n",
      "308/308 [==============================] - 55s 178ms/step - loss: 0.0428 - acc: 0.9877 - val_loss: 1.4756 - val_acc: 0.8115\n",
      "Epoch 200/300\n",
      "308/308 [==============================] - 55s 179ms/step - loss: 0.0396 - acc: 0.9884 - val_loss: 1.6591 - val_acc: 0.7999\n",
      "Epoch 201/300\n",
      "308/308 [==============================] - 55s 178ms/step - loss: 0.0411 - acc: 0.9876 - val_loss: 1.4745 - val_acc: 0.7964\n",
      "Epoch 202/300\n",
      "308/308 [==============================] - 55s 179ms/step - loss: 0.0409 - acc: 0.9881 - val_loss: 1.5332 - val_acc: 0.7968\n",
      "Epoch 203/300\n",
      "308/308 [==============================] - 55s 179ms/step - loss: 0.0478 - acc: 0.9872 - val_loss: 1.8536 - val_acc: 0.7924\n",
      "Epoch 204/300\n",
      "308/308 [==============================] - 55s 178ms/step - loss: 0.0467 - acc: 0.9872 - val_loss: 1.8032 - val_acc: 0.7985\n",
      "Epoch 205/300\n",
      "308/308 [==============================] - 55s 179ms/step - loss: 0.0439 - acc: 0.9880 - val_loss: 1.7334 - val_acc: 0.7977\n",
      "Epoch 206/300\n",
      "308/308 [==============================] - 55s 180ms/step - loss: 0.0415 - acc: 0.9881 - val_loss: 1.4133 - val_acc: 0.8066\n",
      "Epoch 207/300\n",
      "308/308 [==============================] - 55s 179ms/step - loss: 0.0372 - acc: 0.9888 - val_loss: 1.6333 - val_acc: 0.8057\n",
      "Epoch 208/300\n",
      "308/308 [==============================] - 55s 180ms/step - loss: 0.0375 - acc: 0.9879 - val_loss: 1.6663 - val_acc: 0.8026\n",
      "Epoch 209/300\n",
      "308/308 [==============================] - 55s 179ms/step - loss: 0.0357 - acc: 0.9887 - val_loss: 1.5652 - val_acc: 0.7986\n",
      "Epoch 210/300\n",
      "308/308 [==============================] - 55s 178ms/step - loss: 0.0397 - acc: 0.9881 - val_loss: 1.8611 - val_acc: 0.7959\n",
      "Epoch 211/300\n",
      "308/308 [==============================] - 55s 177ms/step - loss: 0.0402 - acc: 0.9887 - val_loss: 1.6142 - val_acc: 0.8101\n",
      "Epoch 212/300\n",
      "308/308 [==============================] - 55s 180ms/step - loss: 0.0443 - acc: 0.9879 - val_loss: 1.5712 - val_acc: 0.8088\n",
      "Epoch 213/300\n",
      "308/308 [==============================] - 55s 179ms/step - loss: 0.0384 - acc: 0.9888 - val_loss: 1.7395 - val_acc: 0.8026\n",
      "Epoch 214/300\n",
      "308/308 [==============================] - 55s 179ms/step - loss: 0.0412 - acc: 0.9883 - val_loss: 1.7718 - val_acc: 0.8101\n",
      "Epoch 215/300\n",
      "308/308 [==============================] - 55s 178ms/step - loss: 0.0362 - acc: 0.9890 - val_loss: 1.8003 - val_acc: 0.8070\n",
      "Epoch 216/300\n",
      "308/308 [==============================] - 55s 179ms/step - loss: 0.0367 - acc: 0.9893 - val_loss: 1.6666 - val_acc: 0.8066\n",
      "Epoch 217/300\n",
      "308/308 [==============================] - 55s 179ms/step - loss: 0.0393 - acc: 0.9882 - val_loss: 1.7644 - val_acc: 0.7995\n",
      "Epoch 218/300\n",
      "308/308 [==============================] - 55s 178ms/step - loss: 0.0365 - acc: 0.9893 - val_loss: 1.9706 - val_acc: 0.8066\n",
      "Epoch 219/300\n",
      "308/308 [==============================] - 55s 179ms/step - loss: 0.0420 - acc: 0.9885 - val_loss: 1.7252 - val_acc: 0.8061\n",
      "Epoch 220/300\n",
      "308/308 [==============================] - 55s 179ms/step - loss: 0.0395 - acc: 0.9888 - val_loss: 1.6859 - val_acc: 0.8101\n",
      "Epoch 221/300\n",
      "308/308 [==============================] - 55s 179ms/step - loss: 0.0417 - acc: 0.9884 - val_loss: 1.8069 - val_acc: 0.8012\n",
      "Epoch 222/300\n",
      "308/308 [==============================] - 55s 178ms/step - loss: 0.0429 - acc: 0.9871 - val_loss: 1.7380 - val_acc: 0.8079\n",
      "Epoch 223/300\n",
      "308/308 [==============================] - 55s 179ms/step - loss: 0.0410 - acc: 0.9883 - val_loss: 1.8827 - val_acc: 0.7981\n",
      "Epoch 224/300\n",
      "308/308 [==============================] - 55s 180ms/step - loss: 0.0435 - acc: 0.9879 - val_loss: 1.7029 - val_acc: 0.8052\n",
      "Epoch 225/300\n",
      "308/308 [==============================] - 55s 179ms/step - loss: 0.0397 - acc: 0.9889 - val_loss: 1.8203 - val_acc: 0.7932\n",
      "Epoch 226/300\n",
      "308/308 [==============================] - 54s 177ms/step - loss: 0.0427 - acc: 0.9875 - val_loss: 1.7657 - val_acc: 0.8079\n",
      "Epoch 227/300\n",
      "308/308 [==============================] - 54s 177ms/step - loss: 0.0399 - acc: 0.9890 - val_loss: 1.8406 - val_acc: 0.7990\n",
      "Epoch 228/300\n",
      "308/308 [==============================] - 55s 179ms/step - loss: 0.0390 - acc: 0.9891 - val_loss: 1.8651 - val_acc: 0.8101\n",
      "Epoch 229/300\n",
      "308/308 [==============================] - 55s 179ms/step - loss: 0.0442 - acc: 0.9880 - val_loss: 1.8114 - val_acc: 0.8026\n",
      "Epoch 230/300\n",
      "308/308 [==============================] - 55s 178ms/step - loss: 0.0453 - acc: 0.9881 - val_loss: 1.6430 - val_acc: 0.7995\n",
      "Epoch 231/300\n",
      "308/308 [==============================] - 55s 177ms/step - loss: 0.0426 - acc: 0.9877 - val_loss: 1.8058 - val_acc: 0.8035\n",
      "Epoch 232/300\n",
      "308/308 [==============================] - 55s 179ms/step - loss: 0.0390 - acc: 0.9897 - val_loss: 1.5927 - val_acc: 0.8110\n",
      "Epoch 233/300\n",
      "308/308 [==============================] - 54s 177ms/step - loss: 0.0400 - acc: 0.9878 - val_loss: 1.6883 - val_acc: 0.8061\n",
      "Epoch 234/300\n",
      "308/308 [==============================] - 55s 179ms/step - loss: 0.0400 - acc: 0.9875 - val_loss: 2.1699 - val_acc: 0.8021\n",
      "Epoch 235/300\n",
      "308/308 [==============================] - 55s 179ms/step - loss: 0.0390 - acc: 0.9894 - val_loss: 1.7763 - val_acc: 0.8141\n",
      "Epoch 236/300\n",
      "308/308 [==============================] - 55s 179ms/step - loss: 0.0450 - acc: 0.9874 - val_loss: 1.6260 - val_acc: 0.8106\n",
      "Epoch 237/300\n",
      "308/308 [==============================] - 55s 177ms/step - loss: 0.0389 - acc: 0.9885 - val_loss: 1.8503 - val_acc: 0.8061\n",
      "Epoch 238/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "308/308 [==============================] - 55s 179ms/step - loss: 0.0395 - acc: 0.9889 - val_loss: 2.1146 - val_acc: 0.8069\n",
      "Epoch 239/300\n",
      "308/308 [==============================] - 55s 178ms/step - loss: 0.0401 - acc: 0.9882 - val_loss: 1.7116 - val_acc: 0.8124\n",
      "Epoch 240/300\n",
      "308/308 [==============================] - 55s 178ms/step - loss: 0.0408 - acc: 0.9881 - val_loss: 1.6720 - val_acc: 0.8075\n",
      "Epoch 241/300\n",
      "308/308 [==============================] - 55s 178ms/step - loss: 0.0407 - acc: 0.9883 - val_loss: 1.6864 - val_acc: 0.8173\n",
      "Epoch 242/300\n",
      "308/308 [==============================] - 55s 178ms/step - loss: 0.0414 - acc: 0.9886 - val_loss: 1.6581 - val_acc: 0.8177\n",
      "Epoch 243/300\n",
      "308/308 [==============================] - 55s 178ms/step - loss: 0.0422 - acc: 0.9890 - val_loss: 1.8311 - val_acc: 0.8026\n",
      "Epoch 244/300\n",
      "308/308 [==============================] - 55s 179ms/step - loss: 0.0393 - acc: 0.9894 - val_loss: 2.1209 - val_acc: 0.7972\n",
      "Epoch 245/300\n",
      "308/308 [==============================] - 55s 179ms/step - loss: 0.0431 - acc: 0.9886 - val_loss: 1.7722 - val_acc: 0.8204\n",
      "Epoch 246/300\n",
      "308/308 [==============================] - 55s 178ms/step - loss: 0.0387 - acc: 0.9895 - val_loss: 1.8489 - val_acc: 0.8146\n",
      "Epoch 247/300\n",
      "308/308 [==============================] - 55s 178ms/step - loss: 0.0412 - acc: 0.9886 - val_loss: 1.4549 - val_acc: 0.8110\n",
      "Epoch 248/300\n",
      "308/308 [==============================] - 55s 178ms/step - loss: 0.0387 - acc: 0.9888 - val_loss: 1.7914 - val_acc: 0.8124\n",
      "Epoch 249/300\n",
      "308/308 [==============================] - 54s 176ms/step - loss: 0.0399 - acc: 0.9889 - val_loss: 1.5884 - val_acc: 0.8052\n",
      "Epoch 250/300\n",
      "308/308 [==============================] - 55s 177ms/step - loss: 0.0370 - acc: 0.9883 - val_loss: 1.7014 - val_acc: 0.8061\n",
      "Epoch 251/300\n",
      "308/308 [==============================] - 55s 178ms/step - loss: 0.0409 - acc: 0.9885 - val_loss: 1.7734 - val_acc: 0.8017\n",
      "Epoch 252/300\n",
      "308/308 [==============================] - 55s 178ms/step - loss: 0.0374 - acc: 0.9895 - val_loss: 1.6712 - val_acc: 0.7990\n",
      "Epoch 253/300\n",
      "308/308 [==============================] - 55s 178ms/step - loss: 0.0387 - acc: 0.9893 - val_loss: 1.7411 - val_acc: 0.8124\n",
      "Epoch 254/300\n",
      "308/308 [==============================] - 55s 178ms/step - loss: 0.0418 - acc: 0.9893 - val_loss: 1.8667 - val_acc: 0.7977\n",
      "Epoch 255/300\n",
      "308/308 [==============================] - 55s 179ms/step - loss: 0.0428 - acc: 0.9884 - val_loss: 1.7888 - val_acc: 0.8012\n",
      "Epoch 256/300\n",
      "308/308 [==============================] - 55s 177ms/step - loss: 0.0381 - acc: 0.9892 - val_loss: 1.7189 - val_acc: 0.8044\n",
      "Epoch 257/300\n",
      "308/308 [==============================] - 55s 179ms/step - loss: 0.0424 - acc: 0.9901 - val_loss: 1.5913 - val_acc: 0.7977\n",
      "Epoch 258/300\n",
      "308/308 [==============================] - 55s 179ms/step - loss: 0.0386 - acc: 0.9897 - val_loss: 1.7556 - val_acc: 0.8070\n",
      "Epoch 259/300\n",
      "308/308 [==============================] - 55s 178ms/step - loss: 0.0405 - acc: 0.9887 - val_loss: 1.6768 - val_acc: 0.8057\n",
      "Epoch 260/300\n",
      "308/308 [==============================] - 55s 177ms/step - loss: 0.0387 - acc: 0.9891 - val_loss: 1.6263 - val_acc: 0.7968\n",
      "Epoch 261/300\n",
      "308/308 [==============================] - 55s 178ms/step - loss: 0.0447 - acc: 0.9890 - val_loss: 1.7308 - val_acc: 0.8075\n",
      "Epoch 262/300\n",
      "308/308 [==============================] - 55s 179ms/step - loss: 0.0419 - acc: 0.9882 - val_loss: 1.5380 - val_acc: 0.8066\n",
      "Epoch 263/300\n",
      "308/308 [==============================] - 55s 177ms/step - loss: 0.0388 - acc: 0.9888 - val_loss: 1.5217 - val_acc: 0.8119\n",
      "Epoch 264/300\n",
      "308/308 [==============================] - 55s 179ms/step - loss: 0.0396 - acc: 0.9889 - val_loss: 1.7695 - val_acc: 0.8088\n",
      "Epoch 265/300\n",
      "308/308 [==============================] - 57s 184ms/step - loss: 0.0445 - acc: 0.9890 - val_loss: 1.7822 - val_acc: 0.8150\n",
      "Epoch 266/300\n",
      "308/308 [==============================] - 55s 177ms/step - loss: 0.0399 - acc: 0.9889 - val_loss: 1.7517 - val_acc: 0.8181\n",
      "Epoch 267/300\n",
      "308/308 [==============================] - 54s 177ms/step - loss: 0.0380 - acc: 0.9897 - val_loss: 1.6126 - val_acc: 0.8088\n",
      "Epoch 268/300\n",
      "308/308 [==============================] - 55s 178ms/step - loss: 0.0470 - acc: 0.9884 - val_loss: 1.7063 - val_acc: 0.8146\n",
      "Epoch 269/300\n",
      "308/308 [==============================] - 55s 178ms/step - loss: 0.0396 - acc: 0.9895 - val_loss: 1.8285 - val_acc: 0.8128\n",
      "Epoch 270/300\n",
      "308/308 [==============================] - 54s 176ms/step - loss: 0.0388 - acc: 0.9883 - val_loss: 1.5841 - val_acc: 0.8137\n",
      "Epoch 271/300\n",
      "308/308 [==============================] - 55s 177ms/step - loss: 0.0432 - acc: 0.9887 - val_loss: 1.5101 - val_acc: 0.8173\n",
      "Epoch 272/300\n",
      "308/308 [==============================] - 55s 179ms/step - loss: 0.0405 - acc: 0.9897 - val_loss: 1.5383 - val_acc: 0.8087\n",
      "Epoch 273/300\n",
      "308/308 [==============================] - 55s 179ms/step - loss: 0.0448 - acc: 0.9895 - val_loss: 1.7086 - val_acc: 0.8168\n",
      "Epoch 274/300\n",
      "308/308 [==============================] - 54s 177ms/step - loss: 0.0430 - acc: 0.9881 - val_loss: 1.5509 - val_acc: 0.8128\n",
      "Epoch 275/300\n",
      "308/308 [==============================] - 55s 180ms/step - loss: 0.0422 - acc: 0.9895 - val_loss: 1.7538 - val_acc: 0.8017\n",
      "Epoch 276/300\n",
      "308/308 [==============================] - 55s 179ms/step - loss: 0.0446 - acc: 0.9887 - val_loss: 1.8279 - val_acc: 0.7999\n",
      "Epoch 277/300\n",
      "308/308 [==============================] - 55s 178ms/step - loss: 0.0393 - acc: 0.9890 - val_loss: 1.7783 - val_acc: 0.8106\n",
      "Epoch 278/300\n",
      "308/308 [==============================] - 55s 178ms/step - loss: 0.0407 - acc: 0.9888 - val_loss: 2.0423 - val_acc: 0.8101\n",
      "Epoch 279/300\n",
      "308/308 [==============================] - 55s 178ms/step - loss: 0.0339 - acc: 0.9901 - val_loss: 1.6512 - val_acc: 0.8110\n",
      "Epoch 280/300\n",
      "308/308 [==============================] - 55s 177ms/step - loss: 0.0373 - acc: 0.9901 - val_loss: 2.0598 - val_acc: 0.7995\n",
      "Epoch 281/300\n",
      "308/308 [==============================] - 55s 178ms/step - loss: 0.0354 - acc: 0.9904 - val_loss: 1.8659 - val_acc: 0.8021\n",
      "Epoch 282/300\n",
      "308/308 [==============================] - 55s 177ms/step - loss: 0.0402 - acc: 0.9890 - val_loss: 1.5854 - val_acc: 0.8097\n",
      "Epoch 283/300\n",
      "308/308 [==============================] - 55s 178ms/step - loss: 0.0397 - acc: 0.9896 - val_loss: 1.9167 - val_acc: 0.8097\n",
      "Epoch 284/300\n",
      "308/308 [==============================] - 55s 178ms/step - loss: 0.0430 - acc: 0.9893 - val_loss: 2.0701 - val_acc: 0.8052\n",
      "Epoch 285/300\n",
      "308/308 [==============================] - 55s 178ms/step - loss: 0.0444 - acc: 0.9889 - val_loss: 1.9536 - val_acc: 0.7990\n",
      "Epoch 286/300\n",
      "308/308 [==============================] - 55s 177ms/step - loss: 0.0409 - acc: 0.9889 - val_loss: 1.7686 - val_acc: 0.8092\n",
      "Epoch 287/300\n",
      "308/308 [==============================] - 55s 178ms/step - loss: 0.0438 - acc: 0.9881 - val_loss: 1.8164 - val_acc: 0.8039\n",
      "Epoch 288/300\n",
      "308/308 [==============================] - 55s 178ms/step - loss: 0.0412 - acc: 0.9893 - val_loss: 1.8663 - val_acc: 0.7941\n",
      "Epoch 289/300\n",
      "308/308 [==============================] - 55s 179ms/step - loss: 0.0371 - acc: 0.9891 - val_loss: 1.9310 - val_acc: 0.8004\n",
      "Epoch 290/300\n",
      "308/308 [==============================] - 55s 179ms/step - loss: 0.0404 - acc: 0.9885 - val_loss: 1.8949 - val_acc: 0.8075\n",
      "Epoch 291/300\n",
      "308/308 [==============================] - 55s 178ms/step - loss: 0.0383 - acc: 0.9890 - val_loss: 1.6525 - val_acc: 0.8141\n",
      "Epoch 292/300\n",
      "308/308 [==============================] - 55s 178ms/step - loss: 0.0428 - acc: 0.9889 - val_loss: 1.7413 - val_acc: 0.8128\n",
      "Epoch 293/300\n",
      "308/308 [==============================] - 55s 178ms/step - loss: 0.0354 - acc: 0.9901 - val_loss: 1.5943 - val_acc: 0.8128\n",
      "Epoch 294/300\n",
      "308/308 [==============================] - 55s 178ms/step - loss: 0.0370 - acc: 0.9895 - val_loss: 1.5918 - val_acc: 0.7964\n",
      "Epoch 295/300\n",
      "308/308 [==============================] - 55s 178ms/step - loss: 0.0379 - acc: 0.9900 - val_loss: 1.8632 - val_acc: 0.8052\n",
      "Epoch 296/300\n",
      "308/308 [==============================] - 55s 178ms/step - loss: 0.0359 - acc: 0.9900 - val_loss: 1.8853 - val_acc: 0.8128\n",
      "Epoch 297/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "308/308 [==============================] - 55s 179ms/step - loss: 0.0388 - acc: 0.9897 - val_loss: 1.9265 - val_acc: 0.7995\n",
      "Epoch 298/300\n",
      "308/308 [==============================] - 55s 177ms/step - loss: 0.0402 - acc: 0.9899 - val_loss: 2.2702 - val_acc: 0.7986\n",
      "Epoch 299/300\n",
      "308/308 [==============================] - 55s 178ms/step - loss: 0.0355 - acc: 0.9895 - val_loss: 2.0218 - val_acc: 0.8066\n",
      "Epoch 300/300\n",
      "308/308 [==============================] - 55s 178ms/step - loss: 0.0425 - acc: 0.9884 - val_loss: 1.6694 - val_acc: 0.8088\n",
      "==========\n"
     ]
    }
   ],
   "source": [
    "# store the results of each setting\n",
    "train_losses = np.zeros(num_settings)\n",
    "dev_losses = np.zeros(num_settings)\n",
    "\n",
    "for setting in range(num_settings):\n",
    "    model = SqueezeNet(include_top=True)\n",
    "    \n",
    "    model.layers.pop()\n",
    "    model.layers.pop()\n",
    "    model.layers.pop()\n",
    "    model.layers.pop()\n",
    "    model.layers.pop()\n",
    "    \n",
    "    # build a classifier model to put on top of the convolutional model\n",
    "    top_model = Sequential()\n",
    "    top_model.add(Convolution2D(256, (1, 1), padding='valid', name='top_conv', input_shape=(model.layers[-1].output_shape[1:])))\n",
    "    top_model.add(AveragePooling2D(pool_size=(5, 5), name='top_avgpool'))\n",
    "    top_model.add(Flatten(input_shape=(model.layers[-1].output_shape[1:]),name='top_flatten'))\n",
    "    top_model.add(Dropout(hp_dropout[setting], name='top_dropout'))\n",
    "#     top_model.add(Dense(hp_hidden[setting], activation='relu', kernel_initializer='glorot_uniform', name='top_dense'))\n",
    "    top_model.add(Dense(4, activation='softmax', name='output', kernel_initializer='glorot_uniform'))\n",
    "    \n",
    "    # add the model on top of the convolutional base\n",
    "    new_model = Model(inputs= model.input, outputs = top_model(model.layers[-1].output))\n",
    "#     new_model = load_model(\"E:/output/bikes-cnn-PriceNet-Class/{epoch:08d}.hdf5\")\n",
    "    new_model.summary()\n",
    "    \n",
    "    # RMSprop optimizer\n",
    "#     new_model.compile(loss='mean_squared_error',\n",
    "#                       optimizer=optimizers.RMSprop(\n",
    "#                               lr=hp_lr[setting], \n",
    "#                               rho=hp_rho[setting], \n",
    "#                               epsilon=hp_epsilon[setting], \n",
    "#                               decay=hp_decay[setting]))\n",
    "    new_model.compile(loss='categorical_crossentropy', optimizer=optimizers.RMSprop(lr=0.0001, rho=0.9, epsilon=1e-07, decay=0.0), metrics=['accuracy'])\n",
    "    \n",
    "    \n",
    "    checkpoint_path = 'E:/output/bikes-cnn-SqueezeNet-Class/{epoch:05d}.hdf5'\n",
    "    \n",
    "    # keep a checkpoint\n",
    "    checkpoint = ModelCheckpoint(checkpoint_path, period=5)\n",
    "    \n",
    "    \n",
    "    minibatch_size = hp_mbsize[setting]\n",
    "\n",
    "    train_steps = math.ceil(len(train_indices) / minibatch_size)\n",
    "    test_steps = math.ceil(len(test_indices) / minibatch_size)\n",
    "\n",
    "    # fine-tune the model\n",
    "    history = new_model.fit_generator(\n",
    "        image_generator(train_indices, minibatch_size),\n",
    "        steps_per_epoch=train_steps,\n",
    "        epochs=num_epochs,\n",
    "        validation_data=image_generator(test_indices, minibatch_size),\n",
    "        nb_val_samples=test_steps,\n",
    "        callbacks=[checkpoint])\n",
    "    \n",
    "   \n",
    "    print(\"==========\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTyAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('Model Acc')\n",
    "plt.ylabel('acc')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['train', 'test'], loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['train', 'test'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        100       0.77      0.84      0.81       496\n",
      "         25       0.80      0.85      0.83       596\n",
      "         50       0.72      0.64      0.67       563\n",
      "         75       0.64      0.62      0.63       530\n",
      "\n",
      "avg / total       0.73      0.74      0.73      2185\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[418   5   4  69]\n",
      " [  2 509  71  14]\n",
      " [  2 100 358 103]\n",
      " [119  19  65 327]]\n"
     ]
    }
   ],
   "source": [
    "new_model = load_model('E:/output/bikes-cnn-SqueezeNet-Class/00300.hdf5')\n",
    "\n",
    "true_label = []\n",
    "predicted_label = []\n",
    "for index in test_indices:\n",
    "    msrp = prices[index]\n",
    "    true_label.append(str(msrp))\n",
    "    \n",
    "    path = image_paths[index]\n",
    "    img = image.load_img(path, target_size=(224, 224))\n",
    "    data = np.expand_dims(image.img_to_array(img), axis=0)\n",
    "    \n",
    "    # Prediction outputs softmax vector\n",
    "    prediction = new_model.predict(data)\n",
    "    \n",
    "    # Set most confident prediction as label, and convert it to our price scale\n",
    "    label = np.argmax(prediction) * 25 + 25\n",
    "    predicted_label.append(str(label))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "print(\"Classification report:\\n%s\\n\"\n",
    "      % (classification_report(true_label, predicted_label)))\n",
    "print(\"Confusion matrix:\\n%s\" % confusion_matrix(true_label, predicted_label))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
