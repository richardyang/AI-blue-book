{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bikes CNN for Price Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "\n",
    "--------------\n",
    "\n",
    "We use Keras to build and train our model, with a Tensorflow backend. We use scikit-learn for evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from keras import applications\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Dropout, Flatten, Dense, Input\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Flatten, Dropout, Concatenate, Activation\n",
    "from keras.layers import Convolution2D, MaxPooling2D, AveragePooling2D\n",
    "from keras.layers import GlobalMaxPooling2D, GlobalAveragePooling2D\n",
    "\n",
    "from keras.applications.imagenet_utils import decode_predictions\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "from keras_applications.imagenet_utils import _obtain_input_shape\n",
    "from keras.utils.data_utils import get_file\n",
    "\n",
    "WEIGHTS_PATH = 'https://github.com/wohlert/keras-squeezenet/releases/download/v0.1/squeezenet_weights.h5'\n",
    "\n",
    "def _fire(x, filters, name=\"fire\"):\n",
    "    sq_filters, ex1_filters, ex2_filters = filters\n",
    "    squeeze = Convolution2D(sq_filters, (1, 1), activation='relu', padding='same', name=name + \"squeeze1x1\")(x)\n",
    "    expand1 = Convolution2D(ex1_filters, (1, 1), activation='relu', padding='same', name=name + \"expand1x1\")(squeeze)\n",
    "    expand2 = Convolution2D(ex2_filters, (3, 3), activation='relu', padding='same', name=name + \"expand3x3\")(squeeze)\n",
    "    x = Concatenate(axis=-1, name=name)([expand1, expand2])\n",
    "    return x\n",
    "\n",
    "def SqueezeNet(include_top=True, weights=\"imagenet\", input_tensor=None, input_shape=None, pooling=None, classes=1000):\n",
    "\n",
    "    if weights not in {'imagenet', None}:\n",
    "        raise ValueError('The `weights` argument should be either '\n",
    "                         '`None` (random initialization) or `imagenet` '\n",
    "                         '(pre-training on ImageNet).')\n",
    "\n",
    "    if weights == 'imagenet' and include_top and classes != 1000:\n",
    "        raise ValueError('If using `weights` as imagenet with `include_top`'\n",
    "                         ' as true, `classes` should be 1000')\n",
    "    # Determine proper input shape\n",
    "    input_shape = _obtain_input_shape(input_shape,\n",
    "                                      default_size=224,\n",
    "                                      min_size=48,\n",
    "                                      data_format=K.image_data_format(),\n",
    "                                      require_flatten=include_top)\n",
    "\n",
    "    if input_tensor is None:\n",
    "        img_input = Input(shape=input_shape)\n",
    "    else:\n",
    "        if not K.is_keras_tensor(input_tensor):\n",
    "            img_input = Input(tensor=input_tensor, shape=input_shape)\n",
    "        else:\n",
    "            img_input = input_tensor\n",
    "\n",
    "    x = Convolution2D(64, kernel_size=(3, 3), strides=(2, 2), padding=\"same\", activation=\"relu\", name='conv1')(img_input)\n",
    "    x = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), name='maxpool1', padding=\"valid\")(x)\n",
    "\n",
    "    x = _fire(x, (16, 64, 64), name=\"fire2\")\n",
    "    x = _fire(x, (16, 64, 64), name=\"fire3\")\n",
    "\n",
    "    x = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), name='maxpool3', padding=\"valid\")(x)\n",
    "\n",
    "    x = _fire(x, (32, 128, 128), name=\"fire4\")\n",
    "    x = _fire(x, (32, 128, 128), name=\"fire5\")\n",
    "\n",
    "    x = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), name='maxpool5', padding=\"valid\")(x)\n",
    "\n",
    "    x = _fire(x, (48, 192, 192), name=\"fire6\")\n",
    "    x = _fire(x, (48, 192, 192), name=\"fire7\")\n",
    "\n",
    "    x = _fire(x, (64, 256, 256), name=\"fire8\")\n",
    "    x = _fire(x, (64, 256, 256), name=\"fire9\")\n",
    "\n",
    "    if include_top:\n",
    "        x = Dropout(0.5, name='dropout9')(x)\n",
    "\n",
    "        x = Convolution2D(classes, (1, 1), padding='valid', name='conv10')(x)\n",
    "        x = AveragePooling2D(pool_size=(13, 13), name='avgpool10')(x)\n",
    "        x = Flatten(name='flatten10')(x)\n",
    "        x = Activation(\"softmax\", name='softmax')(x)\n",
    "    else:\n",
    "        if pooling == \"avg\":\n",
    "            x = GlobalAveragePooling2D(name=\"avgpool10\")(x)\n",
    "        else:\n",
    "            x = GlobalMaxPooling2D(name=\"maxpool10\")(x)\n",
    "\n",
    "    model = Model(img_input, x, name=\"squeezenet\")\n",
    "\n",
    "    if weights == 'imagenet':\n",
    "        weights_path = get_file('squeezenet_weights.h5',\n",
    "                                WEIGHTS_PATH,\n",
    "                                cache_subdir='models')\n",
    "\n",
    "        model.load_weights(weights_path)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data on file paths and prices, as well as train/test split\n",
    "\n",
    "--------------------\n",
    "\n",
    "We load the locations of the images, as well as the train/test split. We use a 90/10 split, which we share across the neural network and baselines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19658,)\n",
      "(2185,)\n"
     ]
    }
   ],
   "source": [
    "# read the CSV into memory\n",
    "prices = []\n",
    "image_paths = []\n",
    "\n",
    "data_path = \"../datasets/bikes_im/\"\n",
    "with open(\"../datasets/bikes_filtered.csv\") as file:\n",
    "    reader = csv.reader(file)\n",
    "    i = -1\n",
    "    for row in reader:\n",
    "        i += 1\n",
    "        index = row[0]\n",
    "        name = row[1]\n",
    "        msrp = row[2]\n",
    "        \n",
    "        image_path = data_path + index + '.jpg'\n",
    "        image_paths.append(image_path)\n",
    "        prices.append(int(msrp))\n",
    "\n",
    "train_indices = np.load(\"bikes_train_indices.npy\")\n",
    "test_indices = np.load(\"bikes_test_indices.npy\")\n",
    "print(train_indices.shape)\n",
    "print(test_indices.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Image Generator\n",
    "\n",
    "Due to the size of our dataset (>20,000 images), we cannot read all images into memory. Thus, we write our own image generator, which is a Python generator that reads images a minibatch at a time, preprocessing them and returning the input data and price labels as input to the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_generator(indices, batch_size):\n",
    "\n",
    "    num_batches = int(len(indices) / batch_size)\n",
    "    \n",
    "    while True:\n",
    "        for batch_i in range(num_batches):\n",
    "            if batch_i == num_batches - 1:\n",
    "                # special case: return as many as possible\n",
    "                start_i = batch_i * batch_size\n",
    "                batch_indices = indices[start_i:]\n",
    "                \n",
    "                X = np.zeros((len(batch_indices), 224, 224, 3))\n",
    "                Y = np.zeros((len(batch_indices), 1))\n",
    "            \n",
    "            else:\n",
    "                start_i = batch_i * batch_size\n",
    "                end_i = start_i + batch_size\n",
    "\n",
    "                batch_indices = indices[start_i:end_i]\n",
    "\n",
    "                X = np.zeros((batch_size, 224, 224, 3))\n",
    "                Y = np.zeros((batch_size, 1))\n",
    "            \n",
    "            for i, index in enumerate(batch_indices):\n",
    "                img = image.load_img(image_paths[index], target_size=(224, 224))\n",
    "                X[i, :, :, :] = image.img_to_array(img)                \n",
    "                Y[i] = prices[index]\n",
    "            \n",
    "            # use vgg16 preprocessing\n",
    "            X = preprocess_input(X)\n",
    "            \n",
    "            yield (X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "\n",
    "We tune hyperparameters using grid search and random search, modifying one hyperparameter at a time while keeping the others constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "num_settings = 1\n",
    "\n",
    "hp_dropout = [0.5] * num_settings\n",
    "\n",
    "#RMSprop\n",
    "hp_lr = [0.001] * num_settings\n",
    "hp_rho = [0.9] * num_settings\n",
    "hp_epsilon = [1e-07] * num_settings\n",
    "hp_decay = [0.0] * num_settings\n",
    "\n",
    "# Number of hidden units\n",
    "hp_hidden = [256] * num_settings\n",
    "\n",
    "# Minibatch size\n",
    "hp_mbsize = [64] * num_settings\n",
    "\n",
    "num_epochs = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 224, 224, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                  (None, 112, 112, 64) 1792        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "maxpool1 (MaxPooling2D)         (None, 55, 55, 64)   0           conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "fire2squeeze1x1 (Conv2D)        (None, 55, 55, 16)   1040        maxpool1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "fire2expand1x1 (Conv2D)         (None, 55, 55, 64)   1088        fire2squeeze1x1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire2expand3x3 (Conv2D)         (None, 55, 55, 64)   9280        fire2squeeze1x1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire2 (Concatenate)             (None, 55, 55, 128)  0           fire2expand1x1[0][0]             \n",
      "                                                                 fire2expand3x3[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "fire3squeeze1x1 (Conv2D)        (None, 55, 55, 16)   2064        fire2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "fire3expand1x1 (Conv2D)         (None, 55, 55, 64)   1088        fire3squeeze1x1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire3expand3x3 (Conv2D)         (None, 55, 55, 64)   9280        fire3squeeze1x1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire3 (Concatenate)             (None, 55, 55, 128)  0           fire3expand1x1[0][0]             \n",
      "                                                                 fire3expand3x3[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "maxpool3 (MaxPooling2D)         (None, 27, 27, 128)  0           fire3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "fire4squeeze1x1 (Conv2D)        (None, 27, 27, 32)   4128        maxpool3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "fire4expand1x1 (Conv2D)         (None, 27, 27, 128)  4224        fire4squeeze1x1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire4expand3x3 (Conv2D)         (None, 27, 27, 128)  36992       fire4squeeze1x1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire4 (Concatenate)             (None, 27, 27, 256)  0           fire4expand1x1[0][0]             \n",
      "                                                                 fire4expand3x3[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "fire5squeeze1x1 (Conv2D)        (None, 27, 27, 32)   8224        fire4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "fire5expand1x1 (Conv2D)         (None, 27, 27, 128)  4224        fire5squeeze1x1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire5expand3x3 (Conv2D)         (None, 27, 27, 128)  36992       fire5squeeze1x1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire5 (Concatenate)             (None, 27, 27, 256)  0           fire5expand1x1[0][0]             \n",
      "                                                                 fire5expand3x3[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "maxpool5 (MaxPooling2D)         (None, 13, 13, 256)  0           fire5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "fire6squeeze1x1 (Conv2D)        (None, 13, 13, 48)   12336       maxpool5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "fire6expand1x1 (Conv2D)         (None, 13, 13, 192)  9408        fire6squeeze1x1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire6expand3x3 (Conv2D)         (None, 13, 13, 192)  83136       fire6squeeze1x1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire6 (Concatenate)             (None, 13, 13, 384)  0           fire6expand1x1[0][0]             \n",
      "                                                                 fire6expand3x3[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "fire7squeeze1x1 (Conv2D)        (None, 13, 13, 48)   18480       fire6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "fire7expand1x1 (Conv2D)         (None, 13, 13, 192)  9408        fire7squeeze1x1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire7expand3x3 (Conv2D)         (None, 13, 13, 192)  83136       fire7squeeze1x1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire7 (Concatenate)             (None, 13, 13, 384)  0           fire7expand1x1[0][0]             \n",
      "                                                                 fire7expand3x3[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "fire8squeeze1x1 (Conv2D)        (None, 13, 13, 64)   24640       fire7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "fire8expand1x1 (Conv2D)         (None, 13, 13, 256)  16640       fire8squeeze1x1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire8expand3x3 (Conv2D)         (None, 13, 13, 256)  147712      fire8squeeze1x1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire8 (Concatenate)             (None, 13, 13, 512)  0           fire8expand1x1[0][0]             \n",
      "                                                                 fire8expand3x3[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "fire9squeeze1x1 (Conv2D)        (None, 13, 13, 64)   32832       fire8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "fire9expand1x1 (Conv2D)         (None, 13, 13, 256)  16640       fire9squeeze1x1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire9expand3x3 (Conv2D)         (None, 13, 13, 256)  147712      fire9squeeze1x1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire9 (Concatenate)             (None, 13, 13, 512)  0           fire9expand1x1[0][0]             \n",
      "                                                                 fire9expand3x3[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "sequential_1 (Sequential)       (None, 1)            393985      fire9[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 1,116,481\n",
      "Trainable params: 1,116,481\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Richard\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:62: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "C:\\Users\\Richard\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:62: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<generator..., validation_steps=35, callbacks=[<keras.ca..., epochs=300, steps_per_epoch=308, validation_data=<generator...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "308/308 [==============================] - 94s 305ms/step - loss: 4906218.5004 - val_loss: 1940621.8808\n",
      "Epoch 2/300\n",
      "308/308 [==============================] - 82s 267ms/step - loss: 2519264.7065 - val_loss: 1580153.8394\n",
      "Epoch 3/300\n",
      "308/308 [==============================] - 78s 254ms/step - loss: 1728969.1523 - val_loss: 1917501.2451\n",
      "Epoch 4/300\n",
      "308/308 [==============================] - 77s 251ms/step - loss: 1389529.2008 - val_loss: 1104396.0382\n",
      "Epoch 5/300\n",
      "308/308 [==============================] - 84s 273ms/step - loss: 1191107.0485 - val_loss: 2000370.0311\n",
      "Epoch 6/300\n",
      "308/308 [==============================] - 88s 286ms/step - loss: 1038381.6844 - val_loss: 914506.1591\n",
      "Epoch 7/300\n",
      "308/308 [==============================] - 83s 271ms/step - loss: 943291.4429 - val_loss: 965434.4108\n",
      "Epoch 8/300\n",
      "308/308 [==============================] - 78s 254ms/step - loss: 826108.7348 - val_loss: 972370.2606\n",
      "Epoch 9/300\n",
      "308/308 [==============================] - 72s 235ms/step - loss: 754837.6659 - val_loss: 1218629.6949\n",
      "Epoch 10/300\n",
      "308/308 [==============================] - 65s 211ms/step - loss: 706856.0366 - val_loss: 1518881.7576\n",
      "Epoch 11/300\n",
      "308/308 [==============================] - 66s 213ms/step - loss: 656456.4066 - val_loss: 891144.4187\n",
      "Epoch 12/300\n",
      "308/308 [==============================] - 62s 201ms/step - loss: 631140.5197 - val_loss: 1200885.5146\n",
      "Epoch 13/300\n",
      "308/308 [==============================] - 72s 233ms/step - loss: 561517.8326 - val_loss: 815057.8241\n",
      "Epoch 14/300\n",
      "308/308 [==============================] - 67s 218ms/step - loss: 567825.1138 - val_loss: 970352.5692\n",
      "Epoch 15/300\n",
      "308/308 [==============================] - 66s 216ms/step - loss: 523787.6356 - val_loss: 853880.2877\n",
      "Epoch 16/300\n",
      "308/308 [==============================] - 65s 210ms/step - loss: 502201.4590 - val_loss: 889696.3175\n",
      "Epoch 17/300\n",
      "308/308 [==============================] - 64s 207ms/step - loss: 502485.0881 - val_loss: 768067.3102\n",
      "Epoch 18/300\n",
      "308/308 [==============================] - 67s 217ms/step - loss: 466285.0563 - val_loss: 845308.5923\n",
      "Epoch 19/300\n",
      "308/308 [==============================] - 64s 209ms/step - loss: 440453.9808 - val_loss: 789521.7363\n",
      "Epoch 20/300\n",
      "308/308 [==============================] - 61s 199ms/step - loss: 432283.7701 - val_loss: 898223.6754\n",
      "Epoch 21/300\n",
      "308/308 [==============================] - 65s 210ms/step - loss: 419078.6304 - val_loss: 785500.5580\n",
      "Epoch 22/300\n",
      "308/308 [==============================] - 73s 238ms/step - loss: 401578.4429 - val_loss: 900937.2545\n",
      "Epoch 23/300\n",
      "308/308 [==============================] - 80s 259ms/step - loss: 392125.4066 - val_loss: 736232.6908\n",
      "Epoch 24/300\n",
      "308/308 [==============================] - 79s 255ms/step - loss: 386472.4538 - val_loss: 758456.8755\n",
      "Epoch 25/300\n",
      "308/308 [==============================] - 66s 213ms/step - loss: 371707.3750 - val_loss: 778490.5602\n",
      "Epoch 26/300\n",
      "308/308 [==============================] - 55s 179ms/step - loss: 370802.6890 - val_loss: 683635.8316\n",
      "Epoch 27/300\n",
      "308/308 [==============================] - 55s 179ms/step - loss: 367084.6686 - val_loss: 690232.9250\n",
      "Epoch 28/300\n",
      "308/308 [==============================] - 55s 177ms/step - loss: 350266.9607 - val_loss: 705399.5581\n",
      "Epoch 29/300\n",
      "308/308 [==============================] - 56s 183ms/step - loss: 350120.2637 - val_loss: 955962.9220\n",
      "Epoch 30/300\n",
      "308/308 [==============================] - 54s 177ms/step - loss: 341598.5363 - val_loss: 754381.3687\n",
      "Epoch 31/300\n",
      "308/308 [==============================] - 54s 174ms/step - loss: 339090.5358 - val_loss: 840291.2471\n",
      "Epoch 32/300\n",
      "308/308 [==============================] - 54s 175ms/step - loss: 327416.1431 - val_loss: 723951.2243\n",
      "Epoch 33/300\n",
      "308/308 [==============================] - 53s 173ms/step - loss: 324543.7793 - val_loss: 667484.5016\n",
      "Epoch 34/300\n",
      "308/308 [==============================] - 54s 174ms/step - loss: 318887.2502 - val_loss: 638731.4422\n",
      "Epoch 35/300\n",
      "308/308 [==============================] - 54s 174ms/step - loss: 317650.3845 - val_loss: 697068.3490\n",
      "Epoch 36/300\n",
      "308/308 [==============================] - 54s 174ms/step - loss: 325272.6495 - val_loss: 1283443.1128\n",
      "Epoch 37/300\n",
      "308/308 [==============================] - 54s 174ms/step - loss: 302213.3119 - val_loss: 612696.2229\n",
      "Epoch 38/300\n",
      "308/308 [==============================] - 53s 173ms/step - loss: 296066.2983 - val_loss: 638382.2489\n",
      "Epoch 39/300\n",
      "308/308 [==============================] - 53s 173ms/step - loss: 298694.7404 - val_loss: 628955.8588\n",
      "Epoch 40/300\n",
      "308/308 [==============================] - 53s 173ms/step - loss: 289994.5296 - val_loss: 700296.1115\n",
      "Epoch 41/300\n",
      "308/308 [==============================] - 53s 173ms/step - loss: 293120.6170 - val_loss: 645203.4737\n",
      "Epoch 42/300\n",
      "308/308 [==============================] - 53s 173ms/step - loss: 296848.4356 - val_loss: 777023.3027\n",
      "Epoch 43/300\n",
      "308/308 [==============================] - 53s 173ms/step - loss: 280236.3494 - val_loss: 647549.5014\n",
      "Epoch 44/300\n",
      "308/308 [==============================] - 53s 172ms/step - loss: 290025.5271 - val_loss: 723579.6601\n",
      "Epoch 45/300\n",
      "308/308 [==============================] - 53s 173ms/step - loss: 280863.9866 - val_loss: 663620.0970\n",
      "Epoch 46/300\n",
      "308/308 [==============================] - 53s 173ms/step - loss: 281307.2087 - val_loss: 693971.3792\n",
      "Epoch 47/300\n",
      "308/308 [==============================] - 53s 173ms/step - loss: 285871.6059 - val_loss: 680460.1705\n",
      "Epoch 48/300\n",
      "308/308 [==============================] - 53s 173ms/step - loss: 284762.2306 - val_loss: 621946.9553\n",
      "Epoch 49/300\n",
      "308/308 [==============================] - 53s 172ms/step - loss: 277930.0461 - val_loss: 655776.9842\n",
      "Epoch 50/300\n",
      "308/308 [==============================] - 53s 172ms/step - loss: 264923.3071 - val_loss: 638569.5491\n",
      "Epoch 51/300\n",
      "308/308 [==============================] - 53s 173ms/step - loss: 267629.8427 - val_loss: 770352.6137\n",
      "Epoch 52/300\n",
      "308/308 [==============================] - 53s 173ms/step - loss: 266023.7550 - val_loss: 702889.2159\n",
      "Epoch 53/300\n",
      "308/308 [==============================] - 53s 173ms/step - loss: 259269.2685 - val_loss: 676974.4294\n",
      "Epoch 54/300\n",
      "308/308 [==============================] - 53s 172ms/step - loss: 260255.8591 - val_loss: 627394.9428\n",
      "Epoch 55/300\n",
      "308/308 [==============================] - 53s 172ms/step - loss: 258558.6201 - val_loss: 667385.5643\n",
      "Epoch 56/300\n",
      "308/308 [==============================] - 53s 173ms/step - loss: 261452.2960 - val_loss: 657716.5845\n",
      "Epoch 57/300\n",
      "308/308 [==============================] - 53s 172ms/step - loss: 260645.7590 - val_loss: 771104.4998\n",
      "Epoch 58/300\n",
      "308/308 [==============================] - 53s 172ms/step - loss: 256706.6830 - val_loss: 707586.0634\n",
      "Epoch 59/300\n",
      "308/308 [==============================] - 53s 173ms/step - loss: 261553.8095 - val_loss: 742538.7154\n",
      "Epoch 60/300\n",
      "308/308 [==============================] - 53s 173ms/step - loss: 241851.1603 - val_loss: 618943.6409\n",
      "Epoch 61/300\n",
      "308/308 [==============================] - 53s 173ms/step - loss: 260273.4616 - val_loss: 709943.2200\n",
      "Epoch 62/300\n",
      "308/308 [==============================] - 53s 172ms/step - loss: 258107.6183 - val_loss: 657758.0759\n",
      "Epoch 63/300\n",
      "308/308 [==============================] - 53s 173ms/step - loss: 236093.5146 - val_loss: 640815.0836\n",
      "Epoch 64/300\n",
      "308/308 [==============================] - 53s 172ms/step - loss: 244434.4258 - val_loss: 670049.4410\n",
      "Epoch 65/300\n",
      "308/308 [==============================] - 53s 173ms/step - loss: 241105.2789 - val_loss: 588924.0832\n",
      "Epoch 66/300\n",
      "308/308 [==============================] - 53s 174ms/step - loss: 248987.3186 - val_loss: 822122.6244\n",
      "Epoch 67/300\n",
      "308/308 [==============================] - 54s 174ms/step - loss: 233383.4280 - val_loss: 611074.7578\n",
      "Epoch 68/300\n",
      "308/308 [==============================] - 53s 174ms/step - loss: 243724.4527 - val_loss: 1049388.2686\n",
      "Epoch 69/300\n",
      "308/308 [==============================] - 54s 174ms/step - loss: 253743.3254 - val_loss: 689591.2223\n",
      "Epoch 70/300\n",
      "308/308 [==============================] - 53s 173ms/step - loss: 252968.8570 - val_loss: 672009.1325\n",
      "Epoch 71/300\n",
      "308/308 [==============================] - 54s 175ms/step - loss: 235086.2709 - val_loss: 638735.6884\n",
      "Epoch 72/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "308/308 [==============================] - 53s 173ms/step - loss: 243758.7040 - val_loss: 790280.5005\n",
      "Epoch 73/300\n",
      "308/308 [==============================] - 53s 172ms/step - loss: 232478.6471 - val_loss: 661563.8272\n",
      "Epoch 74/300\n",
      "308/308 [==============================] - 53s 172ms/step - loss: 252354.1936 - val_loss: 623453.1365\n",
      "Epoch 75/300\n",
      "308/308 [==============================] - 53s 173ms/step - loss: 241644.9295 - val_loss: 590678.5468\n",
      "Epoch 76/300\n",
      "308/308 [==============================] - 53s 172ms/step - loss: 240760.7572 - val_loss: 662943.4909\n",
      "Epoch 77/300\n",
      "308/308 [==============================] - 53s 173ms/step - loss: 228622.4301 - val_loss: 885735.5572\n",
      "Epoch 78/300\n",
      "308/308 [==============================] - 54s 174ms/step - loss: 237463.3149 - val_loss: 707840.7057\n",
      "Epoch 79/300\n",
      "308/308 [==============================] - 54s 174ms/step - loss: 244298.9113 - val_loss: 627705.9075\n",
      "Epoch 80/300\n",
      "308/308 [==============================] - 54s 174ms/step - loss: 244355.9951 - val_loss: 712411.2096\n",
      "Epoch 81/300\n",
      "308/308 [==============================] - 53s 173ms/step - loss: 235868.8246 - val_loss: 649710.3199\n",
      "Epoch 82/300\n",
      "308/308 [==============================] - 54s 174ms/step - loss: 224673.9900 - val_loss: 594465.8558\n",
      "Epoch 83/300\n",
      "308/308 [==============================] - 54s 174ms/step - loss: 244789.0629 - val_loss: 658714.9790\n",
      "Epoch 84/300\n",
      "308/308 [==============================] - 53s 174ms/step - loss: 222521.9416 - val_loss: 601284.7737\n",
      "Epoch 85/300\n",
      "308/308 [==============================] - 54s 175ms/step - loss: 229770.5701 - val_loss: 678262.6791\n",
      "Epoch 86/300\n",
      "308/308 [==============================] - 54s 175ms/step - loss: 245290.3938 - val_loss: 698800.2508\n",
      "Epoch 87/300\n",
      "308/308 [==============================] - 54s 174ms/step - loss: 239701.1841 - val_loss: 638695.7894\n",
      "Epoch 88/300\n",
      "308/308 [==============================] - 53s 173ms/step - loss: 215760.5792 - val_loss: 635861.6796\n",
      "Epoch 89/300\n",
      "308/308 [==============================] - 54s 174ms/step - loss: 216856.8944 - val_loss: 553192.9870\n",
      "Epoch 90/300\n",
      "308/308 [==============================] - 54s 175ms/step - loss: 216289.5431 - val_loss: 577148.0538\n",
      "Epoch 91/300\n",
      "308/308 [==============================] - 54s 176ms/step - loss: 293754.9828 - val_loss: 785011.2691\n",
      "Epoch 92/300\n",
      "308/308 [==============================] - 54s 175ms/step - loss: 235093.5493 - val_loss: 667589.6130\n",
      "Epoch 93/300\n",
      "308/308 [==============================] - 54s 175ms/step - loss: 233240.2974 - val_loss: 557908.3915\n",
      "Epoch 94/300\n",
      "308/308 [==============================] - 54s 176ms/step - loss: 208842.4981 - val_loss: 597734.7408\n",
      "Epoch 95/300\n",
      "308/308 [==============================] - 54s 176ms/step - loss: 208269.7211 - val_loss: 575400.0845\n",
      "Epoch 96/300\n",
      "308/308 [==============================] - 54s 176ms/step - loss: 204657.9056 - val_loss: 558041.5484\n",
      "Epoch 97/300\n",
      "308/308 [==============================] - 54s 176ms/step - loss: 212134.6338 - val_loss: 613327.7713\n",
      "Epoch 98/300\n",
      "308/308 [==============================] - 54s 176ms/step - loss: 201584.2086 - val_loss: 591769.8753\n",
      "Epoch 99/300\n",
      "308/308 [==============================] - 54s 175ms/step - loss: 216870.0809 - val_loss: 609651.5571\n",
      "Epoch 100/300\n",
      "308/308 [==============================] - 54s 176ms/step - loss: 188893.0296 - val_loss: 682941.0959\n",
      "Epoch 101/300\n",
      "308/308 [==============================] - 54s 175ms/step - loss: 196005.1950 - val_loss: 984874.3063\n",
      "Epoch 102/300\n",
      "308/308 [==============================] - 54s 174ms/step - loss: 205830.9347 - val_loss: 562512.2157\n",
      "Epoch 103/300\n",
      "308/308 [==============================] - 54s 174ms/step - loss: 205532.9296 - val_loss: 649326.3586\n",
      "Epoch 104/300\n",
      "308/308 [==============================] - 54s 174ms/step - loss: 201090.9670 - val_loss: 627613.0197\n",
      "Epoch 105/300\n",
      "308/308 [==============================] - 54s 174ms/step - loss: 200541.4620 - val_loss: 546191.7675\n",
      "Epoch 106/300\n",
      "308/308 [==============================] - 53s 173ms/step - loss: 205043.5656 - val_loss: 563665.7972\n",
      "Epoch 107/300\n",
      "308/308 [==============================] - 53s 173ms/step - loss: 198652.3689 - val_loss: 590452.6640\n",
      "Epoch 108/300\n",
      "308/308 [==============================] - 53s 173ms/step - loss: 193791.0222 - val_loss: 650764.5814\n",
      "Epoch 109/300\n",
      "308/308 [==============================] - 53s 173ms/step - loss: 203759.0035 - val_loss: 569928.9690\n",
      "Epoch 110/300\n",
      "308/308 [==============================] - 53s 173ms/step - loss: 191665.8990 - val_loss: 601417.5272\n",
      "Epoch 111/300\n",
      "308/308 [==============================] - 53s 172ms/step - loss: 183829.4402 - val_loss: 547293.4874\n",
      "Epoch 112/300\n",
      "308/308 [==============================] - 53s 173ms/step - loss: 196911.5671 - val_loss: 742913.6310\n",
      "Epoch 113/300\n",
      "308/308 [==============================] - 53s 172ms/step - loss: 200591.5961 - val_loss: 630260.9667\n",
      "Epoch 114/300\n",
      "308/308 [==============================] - 53s 173ms/step - loss: 207932.7513 - val_loss: 577234.6581\n",
      "Epoch 115/300\n",
      "308/308 [==============================] - 53s 173ms/step - loss: 198574.8423 - val_loss: 793324.5728\n",
      "Epoch 116/300\n",
      "308/308 [==============================] - 53s 173ms/step - loss: 213252.6273 - val_loss: 755798.8458\n",
      "Epoch 117/300\n",
      "308/308 [==============================] - 53s 173ms/step - loss: 212925.6697 - val_loss: 879302.9611\n",
      "Epoch 118/300\n",
      "308/308 [==============================] - 53s 173ms/step - loss: 217702.0195 - val_loss: 626281.3409\n",
      "Epoch 119/300\n",
      "308/308 [==============================] - 53s 172ms/step - loss: 233067.0292 - val_loss: 603569.9466\n",
      "Epoch 120/300\n",
      "308/308 [==============================] - 53s 172ms/step - loss: 190399.9351 - val_loss: 520475.6102\n",
      "Epoch 121/300\n",
      "308/308 [==============================] - 53s 172ms/step - loss: 202580.9321 - val_loss: 576988.1822\n",
      "Epoch 122/300\n",
      "308/308 [==============================] - 53s 173ms/step - loss: 191346.9710 - val_loss: 574397.5369\n",
      "Epoch 123/300\n",
      "308/308 [==============================] - 53s 172ms/step - loss: 203542.1091 - val_loss: 649768.1836\n",
      "Epoch 124/300\n",
      "308/308 [==============================] - 53s 171ms/step - loss: 190533.6452 - val_loss: 816781.8475\n",
      "Epoch 125/300\n",
      "308/308 [==============================] - 53s 172ms/step - loss: 202083.2568 - val_loss: 653288.8012\n",
      "Epoch 126/300\n",
      "308/308 [==============================] - 53s 173ms/step - loss: 197572.5174 - val_loss: 564656.7323\n",
      "Epoch 127/300\n",
      "308/308 [==============================] - 53s 173ms/step - loss: 197177.3846 - val_loss: 533097.2749\n",
      "Epoch 128/300\n",
      "308/308 [==============================] - 53s 173ms/step - loss: 221374.9849 - val_loss: 541088.4736\n",
      "Epoch 129/300\n",
      "308/308 [==============================] - 53s 173ms/step - loss: 210516.5958 - val_loss: 566659.5316\n",
      "Epoch 130/300\n",
      "308/308 [==============================] - 56s 183ms/step - loss: 195850.4186 - val_loss: 533086.4480\n",
      "Epoch 131/300\n",
      "308/308 [==============================] - 60s 195ms/step - loss: 186869.6729 - val_loss: 597425.5890\n",
      "Epoch 132/300\n",
      "308/308 [==============================] - 55s 179ms/step - loss: 201617.4724 - val_loss: 584733.1528\n",
      "Epoch 133/300\n",
      "308/308 [==============================] - 53s 173ms/step - loss: 222459.3557 - val_loss: 569112.2650\n",
      "Epoch 134/300\n",
      "308/308 [==============================] - 55s 178ms/step - loss: 182365.5121 - val_loss: 581370.1365\n",
      "Epoch 135/300\n",
      "308/308 [==============================] - 68s 221ms/step - loss: 189230.7606 - val_loss: 724240.1974\n",
      "Epoch 136/300\n",
      "308/308 [==============================] - 71s 232ms/step - loss: 192125.3123 - val_loss: 862339.4257\n",
      "Epoch 137/300\n",
      "271/308 [=========================>....] - ETA: 7s - loss: 177368.5083"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-0364814edaca>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mimage_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_indices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mminibatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m         \u001b[0mnb_val_samples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m         callbacks=[checkpoint])\n\u001b[0m\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[0;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1413\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1414\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1415\u001b[1;33m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1416\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1417\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m    211\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[0;32m    212\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 213\u001b[1;33m                                             class_weight=class_weight)\n\u001b[0m\u001b[0;32m    214\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1213\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1214\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1215\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1216\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1217\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2670\u001b[0m                     \u001b[1;34m'In order to feed symbolic tensors to a Keras model '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2671\u001b[0m                     'in TensorFlow, you need tensorflow 1.8 or higher.')\n\u001b[1;32m-> 2672\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2673\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_legacy_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2652\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2653\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m-> 2654\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2655\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2656\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    903\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 905\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    906\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1107\u001b[0m             \u001b[0mfeed_handles\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msubfeed_val\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1108\u001b[0m           \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1109\u001b[1;33m             \u001b[0mnp_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubfeed_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msubfeed_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1110\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1111\u001b[0m           if (not is_tensor_handle_feed and\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\numpy\\core\\numeric.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m    490\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m     \"\"\"\n\u001b[1;32m--> 492\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    493\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    494\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# store the results of each setting\n",
    "train_losses = np.zeros(num_settings)\n",
    "dev_losses = np.zeros(num_settings)\n",
    "\n",
    "for setting in range(num_settings):\n",
    "    model = SqueezeNet(include_top=True)\n",
    "    \n",
    "    model.layers.pop()\n",
    "    model.layers.pop()\n",
    "    model.layers.pop()\n",
    "    model.layers.pop()\n",
    "    model.layers.pop()\n",
    "    \n",
    "    # build a classifier model to put on top of the convolutional model\n",
    "    top_model = Sequential()\n",
    "    top_model.add(Convolution2D(256, (1, 1), padding='valid', name='top_conv', input_shape=(model.layers[-1].output_shape[1:])))\n",
    "    top_model.add(AveragePooling2D(pool_size=(5, 5), name='top_avgpool'))\n",
    "    top_model.add(Flatten(input_shape=(model.layers[-1].output_shape[1:]),name='top_flatten'))\n",
    "    top_model.add(Dropout(hp_dropout[setting], name='top_dropout'))\n",
    "    top_model.add(Dense(hp_hidden[setting], activation='relu', kernel_initializer='glorot_uniform', name='top_dense'))\n",
    "    top_model.add(Dense(1, activation='linear', name='output', kernel_initializer='glorot_uniform'))\n",
    "    \n",
    "    # add the model on top of the convolutional base\n",
    "    new_model = Model(inputs= model.input, outputs = top_model(model.layers[-1].output))\n",
    "    \n",
    "    # set the first 19 layers (up to the last conv block)\n",
    "    # to non-trainable (weights will not be updated)\n",
    "#     for layer in new_model.layers[:-1]:\n",
    "#         layer.trainable = False\n",
    "    \n",
    "    new_model.summary()\n",
    "    \n",
    "    # RMSprop optimizer\n",
    "    new_model.compile(loss='mean_squared_error',\n",
    "                      optimizer=optimizers.RMSprop(\n",
    "                              lr=hp_lr[setting], \n",
    "                              rho=hp_rho[setting], \n",
    "                              epsilon=hp_epsilon[setting], \n",
    "                              decay=hp_decay[setting]))\n",
    "    \n",
    "    checkpoint_path = 'output/bikes-cnn-SqueezeNet_Vanilla-best.hdf5'\n",
    "    \n",
    "    # keep a checkpoint\n",
    "    checkpoint = ModelCheckpoint(checkpoint_path,\n",
    "                                monitor='val_loss',\n",
    "                                save_best_only=True,\n",
    "                                mode='min')\n",
    "    \n",
    "    \n",
    "    minibatch_size = hp_mbsize[setting]\n",
    "\n",
    "    train_steps = math.ceil(len(train_indices) / minibatch_size)\n",
    "    test_steps = math.ceil(len(test_indices) / minibatch_size)\n",
    "\n",
    "    # fine-tune the model\n",
    "    history = new_model.fit_generator(\n",
    "        image_generator(train_indices, minibatch_size),\n",
    "        steps_per_epoch=train_steps,\n",
    "        epochs=num_epochs,\n",
    "        validation_data=image_generator(test_indices, minibatch_size),\n",
    "        nb_val_samples=test_steps,\n",
    "        callbacks=[checkpoint])\n",
    "    \n",
    "   \n",
    "    print(\"==========\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the training and dev losses for the last epoch (current model)\n",
    "train_losses[setting] = history.history['loss'][-1]\n",
    "dev_losses[setting] = history.history['val_loss'][-1]\n",
    "\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['train', 'test'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions on each batch yielded the validation generator.\n",
    "\n",
    "validation_generator = image_generator(test_indices, minibatch_size)\n",
    "\n",
    "predicted = []\n",
    "actual = []\n",
    "\n",
    "for step in range(test_steps):\n",
    "    X, Y = next(validation_generator)\n",
    "    curr_pred = new_model.predict(X)\n",
    "    for entry in curr_pred:\n",
    "        predicted.append(entry)\n",
    "    for entry in Y:\n",
    "        actual.append(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = np.array(predicted)\n",
    "actual = np.array(actual)\n",
    "\n",
    "MSE = mean_squared_error(predicted, actual)\n",
    "MAE = mean_absolute_error(predicted, actual)\n",
    "R2 = r2_score(actual, predicted)\n",
    "\n",
    "print((MSE, MAE, R2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
